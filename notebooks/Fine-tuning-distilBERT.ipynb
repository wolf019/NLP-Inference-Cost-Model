{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6d3facdb-b335-4561-a244-a9075a4fdc04",
   "metadata": {},
   "source": [
    "The aim of this notebook is to fine tune a distilBERT pretrained model for text classification.\n",
    "\n",
    "- **Model:** https://huggingface.co/distilbert-base-uncased-finetuned-sst-2-english\n",
    "- **Dataset:** https://huggingface.co/datasets/ag_news\n",
    "\n",
    "**Guide**: https://huggingface.co/docs/transformers/traininghttps://huggingface.co/docs/transformers/training\n",
    "\n",
    "**Authors**\n",
    "\n",
    "    - Tom Axberg (taxberg@kth.se)\n",
    "    - Antonio Nieto (antonio.nieto@datatonic.com)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bea94dc3-c0ac-4965-9b08-54ea6050bc37",
   "metadata": {},
   "source": [
    "# Environment setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c9a503ae-59f5-4c58-ae79-766ca124396c",
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in /Users/tomaxberg/opt/anaconda3/envs/nlp_energy_project/lib/python3.7/site-packages (4.16.2)\n",
      "Collecting datasets\n",
      "  Downloading datasets-2.0.0-py3-none-any.whl (325 kB)\n",
      "\u001b[K     |████████████████████████████████| 325 kB 13.7 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: numpy in /Users/tomaxberg/opt/anaconda3/envs/nlp_energy_project/lib/python3.7/site-packages (1.19.2)\n",
      "Collecting torch\n",
      "  Downloading torch-1.11.0-cp37-none-macosx_10_9_x86_64.whl (129.9 MB)\n",
      "\u001b[K     |████████████████████████████████| 129.9 MB 56.6 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: tensorflow in /Users/tomaxberg/opt/anaconda3/envs/nlp_energy_project/lib/python3.7/site-packages (2.7.1)\n",
      "Collecting ipywidgets\n",
      "  Downloading ipywidgets-7.7.0-py2.py3-none-any.whl (123 kB)\n",
      "\u001b[K     |████████████████████████████████| 123 kB 53.3 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: sacremoses in /Users/tomaxberg/opt/anaconda3/envs/nlp_energy_project/lib/python3.7/site-packages (from transformers) (0.0.49)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /Users/tomaxberg/opt/anaconda3/envs/nlp_energy_project/lib/python3.7/site-packages (from transformers) (2022.3.15)\n",
      "Requirement already satisfied: tokenizers!=0.11.3,>=0.10.1 in /Users/tomaxberg/opt/anaconda3/envs/nlp_energy_project/lib/python3.7/site-packages (from transformers) (0.11.6)\n",
      "Requirement already satisfied: filelock in /Users/tomaxberg/opt/anaconda3/envs/nlp_energy_project/lib/python3.7/site-packages (from transformers) (3.6.0)\n",
      "Requirement already satisfied: packaging>=20.0 in /Users/tomaxberg/opt/anaconda3/envs/nlp_energy_project/lib/python3.7/site-packages (from transformers) (21.3)\n",
      "Requirement already satisfied: importlib-metadata in /Users/tomaxberg/opt/anaconda3/envs/nlp_energy_project/lib/python3.7/site-packages (from transformers) (4.11.3)\n",
      "Requirement already satisfied: requests in /Users/tomaxberg/opt/anaconda3/envs/nlp_energy_project/lib/python3.7/site-packages (from transformers) (2.27.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /Users/tomaxberg/opt/anaconda3/envs/nlp_energy_project/lib/python3.7/site-packages (from transformers) (6.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.1.0 in /Users/tomaxberg/opt/anaconda3/envs/nlp_energy_project/lib/python3.7/site-packages (from transformers) (0.4.0)\n",
      "Requirement already satisfied: tqdm>=4.27 in /Users/tomaxberg/opt/anaconda3/envs/nlp_energy_project/lib/python3.7/site-packages (from transformers) (4.63.1)\n",
      "Collecting fsspec[http]>=2021.05.0\n",
      "  Downloading fsspec-2022.3.0-py3-none-any.whl (136 kB)\n",
      "\u001b[K     |████████████████████████████████| 136 kB 58.7 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting aiohttp\n",
      "  Downloading aiohttp-3.8.1-cp37-cp37m-macosx_10_9_x86_64.whl (570 kB)\n",
      "\u001b[K     |████████████████████████████████| 570 kB 51.2 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting pyarrow>=5.0.0\n",
      "  Downloading pyarrow-7.0.0-cp37-cp37m-macosx_10_13_x86_64.whl (20.2 MB)\n",
      "\u001b[K     |████████████████████████████████| 20.2 MB 16.5 MB/s eta 0:00:01     |██████████████████████████▋     | 16.8 MB 16.5 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting multiprocess\n",
      "  Downloading multiprocess-0.70.12.2-py37-none-any.whl (112 kB)\n",
      "\u001b[K     |████████████████████████████████| 112 kB 50.3 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting dill\n",
      "  Using cached dill-0.3.4-py2.py3-none-any.whl (86 kB)\n",
      "Collecting responses<0.19\n",
      "  Downloading responses-0.18.0-py3-none-any.whl (38 kB)\n",
      "Collecting xxhash\n",
      "  Downloading xxhash-3.0.0-cp37-cp37m-macosx_10_9_x86_64.whl (34 kB)\n",
      "Collecting pandas\n",
      "  Using cached pandas-1.3.5-cp37-cp37m-macosx_10_9_x86_64.whl (11.0 MB)\n",
      "Requirement already satisfied: typing-extensions in /Users/tomaxberg/opt/anaconda3/envs/nlp_energy_project/lib/python3.7/site-packages (from torch) (4.1.1)\n",
      "Requirement already satisfied: tensorboard~=2.6 in /Users/tomaxberg/opt/anaconda3/envs/nlp_energy_project/lib/python3.7/site-packages (from tensorflow) (2.8.0)\n",
      "Requirement already satisfied: protobuf>=3.9.2 in /Users/tomaxberg/opt/anaconda3/envs/nlp_energy_project/lib/python3.7/site-packages (from tensorflow) (3.19.4)\n",
      "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.21.0 in /Users/tomaxberg/opt/anaconda3/envs/nlp_energy_project/lib/python3.7/site-packages (from tensorflow) (0.24.0)\n",
      "Requirement already satisfied: wrapt>=1.11.0 in /Users/tomaxberg/opt/anaconda3/envs/nlp_energy_project/lib/python3.7/site-packages (from tensorflow) (1.14.0)\n",
      "Requirement already satisfied: gast<0.5.0,>=0.2.1 in /Users/tomaxberg/opt/anaconda3/envs/nlp_energy_project/lib/python3.7/site-packages (from tensorflow) (0.4.0)\n",
      "Requirement already satisfied: absl-py>=0.4.0 in /Users/tomaxberg/opt/anaconda3/envs/nlp_energy_project/lib/python3.7/site-packages (from tensorflow) (1.0.0)\n",
      "Requirement already satisfied: astunparse>=1.6.0 in /Users/tomaxberg/opt/anaconda3/envs/nlp_energy_project/lib/python3.7/site-packages (from tensorflow) (1.6.3)\n",
      "Requirement already satisfied: h5py>=2.9.0 in /Users/tomaxberg/opt/anaconda3/envs/nlp_energy_project/lib/python3.7/site-packages (from tensorflow) (3.6.0)\n",
      "Requirement already satisfied: keras<2.8,>=2.7.0rc0 in /Users/tomaxberg/opt/anaconda3/envs/nlp_energy_project/lib/python3.7/site-packages (from tensorflow) (2.7.0)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /Users/tomaxberg/opt/anaconda3/envs/nlp_energy_project/lib/python3.7/site-packages (from tensorflow) (1.45.0)\n",
      "Requirement already satisfied: wheel<1.0,>=0.32.0 in /Users/tomaxberg/opt/anaconda3/envs/nlp_energy_project/lib/python3.7/site-packages (from tensorflow) (0.37.1)\n",
      "Requirement already satisfied: six>=1.12.0 in /Users/tomaxberg/opt/anaconda3/envs/nlp_energy_project/lib/python3.7/site-packages (from tensorflow) (1.16.0)\n",
      "Requirement already satisfied: flatbuffers<3.0,>=1.12 in /Users/tomaxberg/opt/anaconda3/envs/nlp_energy_project/lib/python3.7/site-packages (from tensorflow) (2.0)\n",
      "Requirement already satisfied: google-pasta>=0.1.1 in /Users/tomaxberg/opt/anaconda3/envs/nlp_energy_project/lib/python3.7/site-packages (from tensorflow) (0.2.0)\n",
      "Requirement already satisfied: libclang>=9.0.1 in /Users/tomaxberg/opt/anaconda3/envs/nlp_energy_project/lib/python3.7/site-packages (from tensorflow) (13.0.0)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in /Users/tomaxberg/opt/anaconda3/envs/nlp_energy_project/lib/python3.7/site-packages (from tensorflow) (3.3.0)\n",
      "Requirement already satisfied: keras-preprocessing>=1.1.1 in /Users/tomaxberg/opt/anaconda3/envs/nlp_energy_project/lib/python3.7/site-packages (from tensorflow) (1.1.2)\n",
      "Requirement already satisfied: tensorflow-estimator<2.8,~=2.7.0rc0 in /Users/tomaxberg/opt/anaconda3/envs/nlp_energy_project/lib/python3.7/site-packages (from tensorflow) (2.7.0)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in /Users/tomaxberg/opt/anaconda3/envs/nlp_energy_project/lib/python3.7/site-packages (from tensorflow) (1.1.0)\n",
      "Requirement already satisfied: ipykernel>=4.5.1 in /Users/tomaxberg/opt/anaconda3/envs/nlp_energy_project/lib/python3.7/site-packages (from ipywidgets) (6.12.1)\n",
      "Requirement already satisfied: traitlets>=4.3.1 in /Users/tomaxberg/opt/anaconda3/envs/nlp_energy_project/lib/python3.7/site-packages (from ipywidgets) (5.1.1)\n",
      "Requirement already satisfied: ipython>=4.0.0 in /Users/tomaxberg/opt/anaconda3/envs/nlp_energy_project/lib/python3.7/site-packages (from ipywidgets) (7.32.0)\n",
      "Collecting nbformat>=4.2.0\n",
      "  Downloading nbformat-5.3.0-py3-none-any.whl (73 kB)\n",
      "\u001b[K     |████████████████████████████████| 73 kB 3.1 MB/s  eta 0:00:01\n",
      "\u001b[?25hCollecting ipython-genutils~=0.2.0\n",
      "  Using cached ipython_genutils-0.2.0-py2.py3-none-any.whl (26 kB)\n",
      "Collecting widgetsnbextension~=3.6.0\n",
      "  Downloading widgetsnbextension-3.6.0-py2.py3-none-any.whl (1.6 MB)\n",
      "\u001b[K     |████████████████████████████████| 1.6 MB 68.0 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting jupyterlab-widgets>=1.0.0\n",
      "  Downloading jupyterlab_widgets-1.1.0-py3-none-any.whl (245 kB)\n",
      "\u001b[K     |████████████████████████████████| 245 kB 35.8 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: cached-property in /Users/tomaxberg/opt/anaconda3/envs/nlp_energy_project/lib/python3.7/site-packages (from h5py>=2.9.0->tensorflow) (1.5.2)\n",
      "Requirement already satisfied: nest-asyncio in /Users/tomaxberg/opt/anaconda3/envs/nlp_energy_project/lib/python3.7/site-packages (from ipykernel>=4.5.1->ipywidgets) (1.5.5)\n",
      "Requirement already satisfied: jupyter-client>=6.1.12 in /Users/tomaxberg/opt/anaconda3/envs/nlp_energy_project/lib/python3.7/site-packages (from ipykernel>=4.5.1->ipywidgets) (7.2.1)\n",
      "Requirement already satisfied: psutil in /Users/tomaxberg/opt/anaconda3/envs/nlp_energy_project/lib/python3.7/site-packages (from ipykernel>=4.5.1->ipywidgets) (5.9.0)\n",
      "Requirement already satisfied: debugpy>=1.0 in /Users/tomaxberg/opt/anaconda3/envs/nlp_energy_project/lib/python3.7/site-packages (from ipykernel>=4.5.1->ipywidgets) (1.5.1)\n",
      "Requirement already satisfied: appnope in /Users/tomaxberg/opt/anaconda3/envs/nlp_energy_project/lib/python3.7/site-packages (from ipykernel>=4.5.1->ipywidgets) (0.1.3)\n",
      "Requirement already satisfied: tornado>=6.1 in /Users/tomaxberg/opt/anaconda3/envs/nlp_energy_project/lib/python3.7/site-packages (from ipykernel>=4.5.1->ipywidgets) (6.1)\n",
      "Requirement already satisfied: matplotlib-inline>=0.1 in /Users/tomaxberg/opt/anaconda3/envs/nlp_energy_project/lib/python3.7/site-packages (from ipykernel>=4.5.1->ipywidgets) (0.1.3)\n",
      "Requirement already satisfied: pygments in /Users/tomaxberg/opt/anaconda3/envs/nlp_energy_project/lib/python3.7/site-packages (from ipython>=4.0.0->ipywidgets) (2.11.2)\n",
      "Requirement already satisfied: pexpect>4.3 in /Users/tomaxberg/opt/anaconda3/envs/nlp_energy_project/lib/python3.7/site-packages (from ipython>=4.0.0->ipywidgets) (4.8.0)\n",
      "Requirement already satisfied: setuptools>=18.5 in /Users/tomaxberg/opt/anaconda3/envs/nlp_energy_project/lib/python3.7/site-packages (from ipython>=4.0.0->ipywidgets) (58.0.4)\n",
      "Requirement already satisfied: backcall in /Users/tomaxberg/opt/anaconda3/envs/nlp_energy_project/lib/python3.7/site-packages (from ipython>=4.0.0->ipywidgets) (0.2.0)\n",
      "Requirement already satisfied: decorator in /Users/tomaxberg/opt/anaconda3/envs/nlp_energy_project/lib/python3.7/site-packages (from ipython>=4.0.0->ipywidgets) (5.1.1)\n",
      "Requirement already satisfied: pickleshare in /Users/tomaxberg/opt/anaconda3/envs/nlp_energy_project/lib/python3.7/site-packages (from ipython>=4.0.0->ipywidgets) (0.7.5)\n",
      "Requirement already satisfied: prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0 in /Users/tomaxberg/opt/anaconda3/envs/nlp_energy_project/lib/python3.7/site-packages (from ipython>=4.0.0->ipywidgets) (3.0.29)\n",
      "Requirement already satisfied: jedi>=0.16 in /Users/tomaxberg/opt/anaconda3/envs/nlp_energy_project/lib/python3.7/site-packages (from ipython>=4.0.0->ipywidgets) (0.18.1)\n",
      "Requirement already satisfied: parso<0.9.0,>=0.8.0 in /Users/tomaxberg/opt/anaconda3/envs/nlp_energy_project/lib/python3.7/site-packages (from jedi>=0.16->ipython>=4.0.0->ipywidgets) (0.8.3)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /Users/tomaxberg/opt/anaconda3/envs/nlp_energy_project/lib/python3.7/site-packages (from jupyter-client>=6.1.12->ipykernel>=4.5.1->ipywidgets) (2.8.2)\n",
      "Requirement already satisfied: entrypoints in /Users/tomaxberg/opt/anaconda3/envs/nlp_energy_project/lib/python3.7/site-packages (from jupyter-client>=6.1.12->ipykernel>=4.5.1->ipywidgets) (0.4)\n",
      "Requirement already satisfied: pyzmq>=22.3 in /Users/tomaxberg/opt/anaconda3/envs/nlp_energy_project/lib/python3.7/site-packages (from jupyter-client>=6.1.12->ipykernel>=4.5.1->ipywidgets) (22.3.0)\n",
      "Requirement already satisfied: jupyter-core>=4.9.2 in /Users/tomaxberg/opt/anaconda3/envs/nlp_energy_project/lib/python3.7/site-packages (from jupyter-client>=6.1.12->ipykernel>=4.5.1->ipywidgets) (4.9.2)\n",
      "Collecting jsonschema>=2.6\n",
      "  Downloading jsonschema-4.4.0-py3-none-any.whl (72 kB)\n",
      "\u001b[K     |████████████████████████████████| 72 kB 5.1 MB/s  eta 0:00:01\n",
      "\u001b[?25hCollecting fastjsonschema\n",
      "  Downloading fastjsonschema-2.15.3-py3-none-any.whl (22 kB)\n",
      "Collecting attrs>=17.4.0\n",
      "  Downloading attrs-21.4.0-py2.py3-none-any.whl (60 kB)\n",
      "\u001b[K     |████████████████████████████████| 60 kB 22.5 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting importlib-resources>=1.4.0\n",
      "  Downloading importlib_resources-5.6.0-py3-none-any.whl (28 kB)\n",
      "Collecting pyrsistent!=0.17.0,!=0.17.1,!=0.17.2,>=0.14.0\n",
      "  Downloading pyrsistent-0.18.1-cp37-cp37m-macosx_10_9_x86_64.whl (68 kB)\n",
      "\u001b[K     |████████████████████████████████| 68 kB 17.8 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: zipp>=3.1.0 in /Users/tomaxberg/opt/anaconda3/envs/nlp_energy_project/lib/python3.7/site-packages (from importlib-resources>=1.4.0->jsonschema>=2.6->nbformat>=4.2.0->ipywidgets) (3.7.0)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /Users/tomaxberg/opt/anaconda3/envs/nlp_energy_project/lib/python3.7/site-packages (from packaging>=20.0->transformers) (3.0.7)\n",
      "Requirement already satisfied: ptyprocess>=0.5 in /Users/tomaxberg/opt/anaconda3/envs/nlp_energy_project/lib/python3.7/site-packages (from pexpect>4.3->ipython>=4.0.0->ipywidgets) (0.7.0)\n",
      "Requirement already satisfied: wcwidth in /Users/tomaxberg/opt/anaconda3/envs/nlp_energy_project/lib/python3.7/site-packages (from prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0->ipython>=4.0.0->ipywidgets) (0.2.5)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/tomaxberg/opt/anaconda3/envs/nlp_energy_project/lib/python3.7/site-packages (from requests->transformers) (2021.10.8)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/tomaxberg/opt/anaconda3/envs/nlp_energy_project/lib/python3.7/site-packages (from requests->transformers) (3.3)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /Users/tomaxberg/opt/anaconda3/envs/nlp_energy_project/lib/python3.7/site-packages (from requests->transformers) (1.26.9)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in /Users/tomaxberg/opt/anaconda3/envs/nlp_energy_project/lib/python3.7/site-packages (from requests->transformers) (2.0.12)\n",
      "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /Users/tomaxberg/opt/anaconda3/envs/nlp_energy_project/lib/python3.7/site-packages (from tensorboard~=2.6->tensorflow) (0.6.1)\n",
      "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /Users/tomaxberg/opt/anaconda3/envs/nlp_energy_project/lib/python3.7/site-packages (from tensorboard~=2.6->tensorflow) (1.8.1)\n",
      "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /Users/tomaxberg/opt/anaconda3/envs/nlp_energy_project/lib/python3.7/site-packages (from tensorboard~=2.6->tensorflow) (0.4.6)\n",
      "Requirement already satisfied: google-auth<3,>=1.6.3 in /Users/tomaxberg/opt/anaconda3/envs/nlp_energy_project/lib/python3.7/site-packages (from tensorboard~=2.6->tensorflow) (2.6.2)\n",
      "Requirement already satisfied: werkzeug>=0.11.15 in /Users/tomaxberg/opt/anaconda3/envs/nlp_energy_project/lib/python3.7/site-packages (from tensorboard~=2.6->tensorflow) (2.0.3)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /Users/tomaxberg/opt/anaconda3/envs/nlp_energy_project/lib/python3.7/site-packages (from tensorboard~=2.6->tensorflow) (3.3.6)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /Users/tomaxberg/opt/anaconda3/envs/nlp_energy_project/lib/python3.7/site-packages (from google-auth<3,>=1.6.3->tensorboard~=2.6->tensorflow) (5.0.0)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in /Users/tomaxberg/opt/anaconda3/envs/nlp_energy_project/lib/python3.7/site-packages (from google-auth<3,>=1.6.3->tensorboard~=2.6->tensorflow) (4.8)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /Users/tomaxberg/opt/anaconda3/envs/nlp_energy_project/lib/python3.7/site-packages (from google-auth<3,>=1.6.3->tensorboard~=2.6->tensorflow) (0.2.8)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in /Users/tomaxberg/opt/anaconda3/envs/nlp_energy_project/lib/python3.7/site-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard~=2.6->tensorflow) (1.3.1)\n",
      "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /Users/tomaxberg/opt/anaconda3/envs/nlp_energy_project/lib/python3.7/site-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard~=2.6->tensorflow) (0.4.8)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in /Users/tomaxberg/opt/anaconda3/envs/nlp_energy_project/lib/python3.7/site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard~=2.6->tensorflow) (3.2.0)\n",
      "Collecting notebook>=4.4.1\n",
      "  Downloading notebook-6.4.10-py3-none-any.whl (9.9 MB)\n",
      "\u001b[K     |████████████████████████████████| 9.9 MB 38.9 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting nbconvert>=5\n",
      "  Downloading nbconvert-6.4.5-py3-none-any.whl (561 kB)\n",
      "\u001b[K     |████████████████████████████████| 561 kB 11.1 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: jinja2 in /Users/tomaxberg/opt/anaconda3/envs/nlp_energy_project/lib/python3.7/site-packages (from notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (3.0.3)\n",
      "Collecting prometheus-client\n",
      "  Downloading prometheus_client-0.14.0-py3-none-any.whl (59 kB)\n",
      "\u001b[K     |████████████████████████████████| 59 kB 20.7 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting Send2Trash>=1.8.0\n",
      "  Using cached Send2Trash-1.8.0-py3-none-any.whl (18 kB)\n",
      "Collecting argon2-cffi\n",
      "  Downloading argon2_cffi-21.3.0-py3-none-any.whl (14 kB)\n",
      "Collecting terminado>=0.8.3\n",
      "  Downloading terminado-0.13.3-py3-none-any.whl (14 kB)\n",
      "Collecting bleach\n",
      "  Using cached bleach-4.1.0-py2.py3-none-any.whl (157 kB)\n",
      "Collecting pandocfilters>=1.4.1\n",
      "  Downloading pandocfilters-1.5.0-py2.py3-none-any.whl (8.7 kB)\n",
      "Collecting mistune<2,>=0.8.1\n",
      "  Using cached mistune-0.8.4-py2.py3-none-any.whl (16 kB)\n",
      "Collecting jupyterlab-pygments\n",
      "  Using cached jupyterlab_pygments-0.1.2-py2.py3-none-any.whl (4.6 kB)\n",
      "Collecting testpath\n",
      "  Downloading testpath-0.6.0-py3-none-any.whl (83 kB)\n",
      "\u001b[K     |████████████████████████████████| 83 kB 9.2 MB/s  eta 0:00:01\n",
      "\u001b[?25hCollecting beautifulsoup4\n",
      "  Using cached beautifulsoup4-4.10.0-py3-none-any.whl (97 kB)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /Users/tomaxberg/opt/anaconda3/envs/nlp_energy_project/lib/python3.7/site-packages (from nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (2.1.1)\n",
      "Collecting nbclient<0.6.0,>=0.5.0\n",
      "  Downloading nbclient-0.5.13-py3-none-any.whl (70 kB)\n",
      "\u001b[K     |████████████████████████████████| 70 kB 15.2 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting defusedxml\n",
      "  Using cached defusedxml-0.7.1-py2.py3-none-any.whl (25 kB)\n",
      "Collecting async-timeout<5.0,>=4.0.0a3\n",
      "  Using cached async_timeout-4.0.2-py3-none-any.whl (5.8 kB)\n",
      "Collecting yarl<2.0,>=1.0\n",
      "  Downloading yarl-1.7.2-cp37-cp37m-macosx_10_9_x86_64.whl (120 kB)\n",
      "\u001b[K     |████████████████████████████████| 120 kB 63.5 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting frozenlist>=1.1.1\n",
      "  Downloading frozenlist-1.3.0-cp37-cp37m-macosx_10_9_x86_64.whl (35 kB)\n",
      "Collecting multidict<7.0,>=4.5\n",
      "  Downloading multidict-6.0.2-cp37-cp37m-macosx_10_9_x86_64.whl (28 kB)\n",
      "Collecting aiosignal>=1.1.2\n",
      "  Using cached aiosignal-1.2.0-py3-none-any.whl (8.2 kB)\n",
      "Collecting asynctest==0.13.0\n",
      "  Downloading asynctest-0.13.0-py3-none-any.whl (26 kB)\n",
      "Collecting argon2-cffi-bindings\n",
      "  Downloading argon2_cffi_bindings-21.2.0-cp36-abi3-macosx_10_9_x86_64.whl (29 kB)\n",
      "Collecting cffi>=1.0.1\n",
      "  Downloading cffi-1.15.0-cp37-cp37m-macosx_10_9_x86_64.whl (178 kB)\n",
      "\u001b[K     |████████████████████████████████| 178 kB 68.2 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting pycparser\n",
      "  Using cached pycparser-2.21-py2.py3-none-any.whl (118 kB)\n",
      "Collecting soupsieve>1.2\n",
      "  Using cached soupsieve-2.3.1-py3-none-any.whl (37 kB)\n",
      "Collecting webencodings\n",
      "  Using cached webencodings-0.5.1-py2.py3-none-any.whl (11 kB)\n",
      "Collecting pytz>=2017.3\n",
      "  Downloading pytz-2022.1-py2.py3-none-any.whl (503 kB)\n",
      "\u001b[K     |████████████████████████████████| 503 kB 61.3 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: joblib in /Users/tomaxberg/opt/anaconda3/envs/nlp_energy_project/lib/python3.7/site-packages (from sacremoses->transformers) (1.1.0)\n",
      "Requirement already satisfied: click in /Users/tomaxberg/opt/anaconda3/envs/nlp_energy_project/lib/python3.7/site-packages (from sacremoses->transformers) (8.0.4)\n",
      "Installing collected packages: pyrsistent, importlib-resources, attrs, pycparser, jsonschema, fastjsonschema, webencodings, soupsieve, nbformat, cffi, testpath, pandocfilters, nbclient, multidict, mistune, jupyterlab-pygments, frozenlist, defusedxml, bleach, beautifulsoup4, argon2-cffi-bindings, yarl, terminado, Send2Trash, prometheus-client, nbconvert, ipython-genutils, asynctest, async-timeout, argon2-cffi, aiosignal, pytz, notebook, fsspec, dill, aiohttp, xxhash, widgetsnbextension, responses, pyarrow, pandas, multiprocess, jupyterlab-widgets, torch, ipywidgets, datasets\n",
      "Successfully installed Send2Trash-1.8.0 aiohttp-3.8.1 aiosignal-1.2.0 argon2-cffi-21.3.0 argon2-cffi-bindings-21.2.0 async-timeout-4.0.2 asynctest-0.13.0 attrs-21.4.0 beautifulsoup4-4.10.0 bleach-4.1.0 cffi-1.15.0 datasets-2.0.0 defusedxml-0.7.1 dill-0.3.4 fastjsonschema-2.15.3 frozenlist-1.3.0 fsspec-2022.3.0 importlib-resources-5.6.0 ipython-genutils-0.2.0 ipywidgets-7.7.0 jsonschema-4.4.0 jupyterlab-pygments-0.1.2 jupyterlab-widgets-1.1.0 mistune-0.8.4 multidict-6.0.2 multiprocess-0.70.12.2 nbclient-0.5.13 nbconvert-6.4.5 nbformat-5.3.0 notebook-6.4.10 pandas-1.3.5 pandocfilters-1.5.0 prometheus-client-0.14.0 pyarrow-7.0.0 pycparser-2.21 pyrsistent-0.18.1 pytz-2022.1 responses-0.18.0 soupsieve-2.3.1 terminado-0.13.3 testpath-0.6.0 torch-1.11.0 webencodings-0.5.1 widgetsnbextension-3.6.0 xxhash-3.0.0 yarl-1.7.2\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install transformers datasets numpy torch tensorflow ipywidgets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2e23e1d-c566-42bd-8e15-6f5e9260ca50",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4f143f1f-0806-4109-844c-40c590af5fb9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/tomaxberg/opt/anaconda3/envs/tf-metal/lib/python3.8/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "import torch\n",
    "import tensorflow as tf\n",
    "from datasets import load_dataset\n",
    "from transformers.file_utils import is_tf_available, is_torch_available\n",
    "from transformers import DistilBertTokenizerFast, TFAutoModelForSequenceClassification\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57aa25e1-7d49-4cc9-bb62-255776a80e16",
   "metadata": {},
   "source": [
    "# Custom functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bdbcfefe-8068-4d9f-9870-7ae6707179a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_seed(seed: int):\n",
    "    \"\"\"\n",
    "    Helper function for reproducible behavior to set the seed in ``random``, ``numpy``, ``torch`` and/or ``tf`` (if\n",
    "    installed).\n",
    "\n",
    "    Args:\n",
    "        seed (:obj:`int`): The seed to set.\n",
    "    \"\"\"\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    if is_torch_available():\n",
    "        torch.manual_seed(seed)\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "        # ^^ safe to call this function even if cuda is not available\n",
    "    if is_tf_available():\n",
    "        import tensorflow as tf\n",
    "\n",
    "        tf.random.set_seed(seed)\n",
    "\n",
    "set_seed(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e4395cc-93e5-4e8f-b4d7-39158c736152",
   "metadata": {},
   "source": [
    "# Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d5ebf08c-c792-496b-b73e-6dafe71c1d9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_name = 'ag_news'\n",
    "num_targets = 4 \n",
    "model_name = \"distilbert-base-uncased\"\n",
    "max_length = 512"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9eca5133-3277-4731-8601-623cad9a2b67",
   "metadata": {},
   "source": [
    "# Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7ecee7cc-d0e3-4828-a21a-cf9a9f00f148",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration default\n",
      "Reusing dataset ag_news (/Users/tomaxberg/.cache/huggingface/datasets/ag_news/default/0.0.0/bc2bcb40336ace1a0374767fc29bb0296cdaf8a6da7298436239c54d79180548)\n",
      "Using custom data configuration default\n",
      "Reusing dataset ag_news (/Users/tomaxberg/.cache/huggingface/datasets/ag_news/default/0.0.0/bc2bcb40336ace1a0374767fc29bb0296cdaf8a6da7298436239c54d79180548)\n",
      "Using custom data configuration default\n",
      "Reusing dataset ag_news (/Users/tomaxberg/.cache/huggingface/datasets/ag_news/default/0.0.0/bc2bcb40336ace1a0374767fc29bb0296cdaf8a6da7298436239c54d79180548)\n"
     ]
    }
   ],
   "source": [
    "# Manually specify the number of unique targets\n",
    "train_dataset = load_dataset(dataset_name, split=\"train[10%:]\")\n",
    "val_dataset = load_dataset(dataset_name, split=\"train[:10%]\")\n",
    "test_dataset = load_dataset(dataset_name, split=\"test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d5a277ae-c053-48d6-9c11-f1b90e3fc646",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['text', 'label'],\n",
       "    num_rows: 108000\n",
       "})"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0004b972-22be-40d9-95cb-9e79add91b4b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'text': 'RocketInfo Partners with Canadian Press, Helps Nascar RocketInfo Partners with Canadian Press, Helps Nascar\\\\\\\\Rocketinfo Inc., news search engine announced yesterday that it has formed a key reseller alliance with the Canadian Press (CP), one of the top-rated multimedia news agencies in the world. CP plans to expand their media monitoring services by offering clients access to the ...',\n",
       " 'label': 3}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75c97026-a0b2-47fd-bed4-7aaf7e354944",
   "metadata": {},
   "source": [
    "# Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7c34d4b3-f443-42cf-b22a-4132ae67f059",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the tokenizer (convert our text to sequence of tokens)\n",
    "tokenizer = DistilBertTokenizerFast.from_pretrained(model_name, do_lower_case=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6fab4359-54a8-4302-a813-555db5918cc3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at /Users/tomaxberg/.cache/huggingface/datasets/ag_news/default/0.0.0/bc2bcb40336ace1a0374767fc29bb0296cdaf8a6da7298436239c54d79180548/cache-b7a2597aa4e1db63.arrow\n",
      "Loading cached processed dataset at /Users/tomaxberg/.cache/huggingface/datasets/ag_news/default/0.0.0/bc2bcb40336ace1a0374767fc29bb0296cdaf8a6da7298436239c54d79180548/cache-b7b942247f78c386.arrow\n",
      "Loading cached processed dataset at /Users/tomaxberg/.cache/huggingface/datasets/ag_news/default/0.0.0/bc2bcb40336ace1a0374767fc29bb0296cdaf8a6da7298436239c54d79180548/cache-a8da77fd8a03c41a.arrow\n"
     ]
    }
   ],
   "source": [
    "# tokenize the dataset, truncate when passed 'max_length' and pad with 0's when less than 'max_length'\n",
    "train_tokenized = train_dataset.map(lambda x: tokenizer(x['text'], truncation=True, padding='max_length'), batched=True)\n",
    "val_tokenized = val_dataset.map(lambda x: tokenizer(x['text'], truncation=True, padding='max_length'), batched=True)\n",
    "test_tokenized = test_dataset.map(lambda x: tokenizer(x['text'], truncation=True, padding='max_length'), batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9d87a331-3221-484b-937c-a28f18f08047",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['text', 'label', 'input_ids', 'attention_mask'],\n",
       "    num_rows: 108000\n",
       "})"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_tokenized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a403808",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c5bbcaae-5195-4511-90aa-65b714c74b52",
   "metadata": {},
   "source": [
    "# Prepare dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "500be08c-7de0-49e8-8cfe-a6148fb8c7b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-05-01 20:28:15.941187: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE4.2 AVX AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-05-01 20:28:15.947911: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:305] Could not identify NUMA node of platform GPU ID 0, defaulting to 0. Your kernel may not have been built with NUMA support.\n",
      "2022-05-01 20:28:15.949346: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:271] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 0 MB memory) -> physical PluggableDevice (device: 0, name: METAL, pci bus id: <undefined>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metal device set to: AMD Radeon Pro 560\n",
      "\n",
      "systemMemory: 16.00 GB\n",
      "maxCacheSize: 2.00 GB\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# get data in standard tf.data.Dataset and remove 'text' label as it is not longer needed\n",
    "tf_train_dataset = train_tokenized.remove_columns(['text']).with_format('tensorflow')\n",
    "tf_val_dataset = val_tokenized.remove_columns(['text']).with_format('tensorflow')\n",
    "tf_test_dataset = test_tokenized.remove_columns(['text']).with_format('tensorflow')\n",
    "\n",
    "# convert to tensors\n",
    "train_features = {x: tf_train_dataset[x] for x in tokenizer.model_input_names}\n",
    "train_tf_dataset = tf.data.Dataset.from_tensor_slices((train_features, tf_train_dataset[\"label\"]))\n",
    "train_tf_dataset = train_tf_dataset.shuffle(len(tf_train_dataset)).batch(8)\n",
    "\n",
    "val_features = {x: tf_val_dataset[x] for x in tokenizer.model_input_names}\n",
    "val_tf_dataset = tf.data.Dataset.from_tensor_slices((val_features, tf_val_dataset[\"label\"]))\n",
    "val_tf_dataset = val_tf_dataset.shuffle(len(val_tf_dataset)).batch(8)\n",
    "\n",
    "test_features = {x: tf_test_dataset[x] for x in tokenizer.model_input_names}\n",
    "test_tf_dataset = tf.data.Dataset.from_tensor_slices((test_features, tf_test_dataset[\"label\"]))\n",
    "test_tf_dataset = test_tf_dataset.shuffle(len(test_tf_dataset)).batch(8)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "165b251f-1e99-4817-b13f-601e44920288",
   "metadata": {},
   "source": [
    "# Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "62df12b4-4b76-42a4-ae52-63e773a61f16",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-05-01 20:28:20.794806: W tensorflow/python/util/util.cc:368] Sets are not currently considered sequences, but this may change in the future, so consider avoiding using them.\n",
      "Some layers from the model checkpoint at distilbert-base-uncased were not used when initializing TFDistilBertForSequenceClassification: ['vocab_projector', 'vocab_transform', 'activation_13', 'vocab_layer_norm']\n",
      "- This IS expected if you are initializing TFDistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFDistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some layers of TFDistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['dropout_19', 'pre_classifier', 'classifier']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# load the model (pre-trained weights)\n",
    "model = TFAutoModelForSequenceClassification.from_pretrained(model_name, num_labels=num_targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "df4a64e4-eb36-4aa3-a455-69706d83a21a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-04-14 15:26:33.068825: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:113] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "ename": "Error",
     "evalue": "Canceled future for execute_request message before replies were done",
     "output_type": "error",
     "traceback": [
      "Error: Canceled future for execute_request message before replies were done",
      "at t.KernelShellFutureHandler.dispose (/Users/tomaxberg/.vscode/extensions/ms-toolsai.jupyter-2022.3.1000901801/out/extension.js:2:1204175)",
      "at /Users/tomaxberg/.vscode/extensions/ms-toolsai.jupyter-2022.3.1000901801/out/extension.js:2:1223227",
      "at Map.forEach (<anonymous>)",
      "at v._clearKernelState (/Users/tomaxberg/.vscode/extensions/ms-toolsai.jupyter-2022.3.1000901801/out/extension.js:2:1223212)",
      "at v.dispose (/Users/tomaxberg/.vscode/extensions/ms-toolsai.jupyter-2022.3.1000901801/out/extension.js:2:1216694)",
      "at /Users/tomaxberg/.vscode/extensions/ms-toolsai.jupyter-2022.3.1000901801/out/extension.js:2:533674",
      "at t.swallowExceptions (/Users/tomaxberg/.vscode/extensions/ms-toolsai.jupyter-2022.3.1000901801/out/extension.js:2:913059)",
      "at dispose (/Users/tomaxberg/.vscode/extensions/ms-toolsai.jupyter-2022.3.1000901801/out/extension.js:2:533652)",
      "at t.RawSession.dispose (/Users/tomaxberg/.vscode/extensions/ms-toolsai.jupyter-2022.3.1000901801/out/extension.js:2:537330)",
      "at runMicrotasks (<anonymous>)",
      "at processTicksAndRejections (node:internal/process/task_queues:96:5)"
     ]
    }
   ],
   "source": [
    "model.compile(\n",
    "    optimizer=tf.keras.optimizers.Adam(learning_rate=5e-5),\n",
    "    loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "    metrics=tf.metrics.SparseCategoricalAccuracy()\n",
    ")\n",
    "model.fit(train_tf_dataset, validation_data=val_tf_dataset, epochs=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "eb5e3761-c734-4486-9f07-742fecd3e3da",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_pretrained(f\"../models/{model_name}-trained\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0a35504-d973-4685-a6c9-99e5ba4d277c",
   "metadata": {},
   "source": [
    "# Load trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "5ad9e29f-e3ad-420b-a147-078e96d25edd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some layers from the model checkpoint at distilbert-base-uncased were not used when initializing TFDistilBertForSequenceClassification: ['vocab_projector', 'vocab_transform', 'activation_13', 'vocab_layer_norm']\n",
      "- This IS expected if you are initializing TFDistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFDistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some layers of TFDistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['pre_classifier', 'dropout_39', 'classifier']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model = TFAutoModelForSequenceClassification.from_pretrained(model_name, num_labels=num_targets)\n",
    "model.load_weights(f\"../models/{model_name}-trained/tf_model.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a1a92147",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metal device set to: AMD Radeon Pro 560\n",
      "\n",
      "systemMemory: 16.00 GB\n",
      "maxCacheSize: 2.00 GB\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-04-14 10:05:56.029307: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE4.2 AVX AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-04-14 10:05:56.031735: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:305] Could not identify NUMA node of platform GPU ID 0, defaulting to 0. Your kernel may not have been built with NUMA support.\n",
      "2022-04-14 10:05:56.032490: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:271] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 0 MB memory) -> physical PluggableDevice (device: 0, name: METAL, pci bus id: <undefined>)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[LogicalDevice(name='/device:CPU:0', device_type='CPU'),\n",
       " LogicalDevice(name='/device:GPU:0', device_type='GPU')]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# model.summary()\n",
    "\n",
    "tf.config.list_logical_devices()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2db9a544-91e5-4776-b7c4-25d88b652068",
   "metadata": {},
   "source": [
    "## Test the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "e35cfec1-8a07-4b9a-8475-a4cba6212df1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The predicted label is: 3\n"
     ]
    }
   ],
   "source": [
    "input_text = \"Up until Python 3.6 a script called pyvenv was also included as a wrapper around venv, but this has been deprecated. It will be completely removed in Python 3.8. The exact same functionality is available when using venv, and any existing documentation should be updated. \"\n",
    "input_text_tokenized = tokenizer.encode(input_text,\n",
    "                                        truncation=True,\n",
    "                                        padding=True,\n",
    "                                        return_tensors=\"tf\")\n",
    "prediction = model(input_text_tokenized)\n",
    "prediction_logits = prediction[0]\n",
    "prediction_probs = tf.nn.softmax(prediction_logits,axis=1).numpy()\n",
    "print(f'The predicted label is: {np.argmax(prediction_probs)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "adccf126-1739-4d78-9c81-312cad9cac80",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_pretrained(f\"../models/{model_name}-trained\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f44a7d3d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-05-01 20:28:53.678520: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:113] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "950/950 [==============================] - 1375s 1s/step - loss: 0.2165 - sparse_categorical_accuracy: 0.9347\n",
      "test loss, test acc: [0.21650908887386322, 0.9347367882728577]\n"
     ]
    }
   ],
   "source": [
    "model.compile(\n",
    "    optimizer=tf.keras.optimizers.Adam(learning_rate=5e-5),\n",
    "    loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "    metrics=tf.metrics.SparseCategoricalAccuracy()\n",
    ")\n",
    "result = model.evaluate(test_tf_dataset)\n",
    "print(\"test loss, test acc:\", result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ac965e0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "python3",
   "name": "tf2-gpu.2-6.m89",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/tf2-gpu.2-6:m89"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
