{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6d3facdb-b335-4561-a244-a9075a4fdc04",
   "metadata": {},
   "source": [
    "The aim of this notebook is to fine tune a BERT pretrained model for text classification.\n",
    "\n",
    "- **Model:** https://huggingface.co/bert-base-uncased\n",
    "- **Dataset:** https://huggingface.co/datasets/ag_news\n",
    "\n",
    "**Guide**: https://huggingface.co/docs/transformers/traininghttps://huggingface.co/docs/transformers/training\n",
    "\n",
    "**Authors**\n",
    "\n",
    "    - Tom Axberg (taxberg@kth.se)\n",
    "    - Antonio Nieto (antonio.nieto@datatonic.com)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bea94dc3-c0ac-4965-9b08-54ea6050bc37",
   "metadata": {},
   "source": [
    "# Environment setup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9a503ae-59f5-4c58-ae79-766ca124396c",
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "source": [
    "!pip install transformers datasets numpy torch tensorflow ipywidgets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2e23e1d-c566-42bd-8e15-6f5e9260ca50",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4f143f1f-0806-4109-844c-40c590af5fb9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/tomaxberg/opt/anaconda3/envs/tf-metal/lib/python3.8/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "import torch\n",
    "import tensorflow as tf\n",
    "from datasets import load_dataset, load_metric\n",
    "from transformers.file_utils import is_tf_available, is_torch_available\n",
    "from transformers import BertTokenizerFast, TFAutoModelForSequenceClassification, Trainer, DataCollatorWithPadding\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57aa25e1-7d49-4cc9-bb62-255776a80e16",
   "metadata": {},
   "source": [
    "# Custom functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bdbcfefe-8068-4d9f-9870-7ae6707179a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_seed(seed: int):\n",
    "    \"\"\"\n",
    "    Helper function for reproducible behavior to set the seed in ``random``, ``numpy``, ``torch`` and/or ``tf`` (if\n",
    "    installed).\n",
    "\n",
    "    Args:\n",
    "        seed (:obj:`int`): The seed to set.\n",
    "    \"\"\"\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    if is_torch_available():\n",
    "        torch.manual_seed(seed)\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "        # ^^ safe to call this function even if cuda is not available\n",
    "    if is_tf_available():\n",
    "        import tensorflow as tf\n",
    "\n",
    "        tf.random.set_seed(seed)\n",
    "\n",
    "set_seed(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e4395cc-93e5-4e8f-b4d7-39158c736152",
   "metadata": {},
   "source": [
    "# Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d5ebf08c-c792-496b-b73e-6dafe71c1d9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_name = 'ag_news'\n",
    "num_targets = 4 \n",
    "model_name = \"bert-large-uncased\"\n",
    "max_length = 512"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9eca5133-3277-4731-8601-623cad9a2b67",
   "metadata": {},
   "source": [
    "# Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7ecee7cc-d0e3-4828-a21a-cf9a9f00f148",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration default\n",
      "Reusing dataset ag_news (/Users/tomaxberg/.cache/huggingface/datasets/ag_news/default/0.0.0/bc2bcb40336ace1a0374767fc29bb0296cdaf8a6da7298436239c54d79180548)\n",
      "Using custom data configuration default\n",
      "Reusing dataset ag_news (/Users/tomaxberg/.cache/huggingface/datasets/ag_news/default/0.0.0/bc2bcb40336ace1a0374767fc29bb0296cdaf8a6da7298436239c54d79180548)\n",
      "Using custom data configuration default\n",
      "Reusing dataset ag_news (/Users/tomaxberg/.cache/huggingface/datasets/ag_news/default/0.0.0/bc2bcb40336ace1a0374767fc29bb0296cdaf8a6da7298436239c54d79180548)\n"
     ]
    }
   ],
   "source": [
    "# Manually specify the number of unique targets\n",
    "train_dataset = load_dataset(dataset_name, split=\"train[10%:]\")\n",
    "val_dataset = load_dataset(dataset_name, split=\"train[:10%]\")\n",
    "test_dataset = load_dataset(dataset_name, split=\"test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d5a277ae-c053-48d6-9c11-f1b90e3fc646",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['text', 'label'],\n",
       "    num_rows: 108000\n",
       "})"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0004b972-22be-40d9-95cb-9e79add91b4b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'text': 'RocketInfo Partners with Canadian Press, Helps Nascar RocketInfo Partners with Canadian Press, Helps Nascar\\\\\\\\Rocketinfo Inc., news search engine announced yesterday that it has formed a key reseller alliance with the Canadian Press (CP), one of the top-rated multimedia news agencies in the world. CP plans to expand their media monitoring services by offering clients access to the ...',\n",
       " 'label': 3}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75c97026-a0b2-47fd-bed4-7aaf7e354944",
   "metadata": {},
   "source": [
    "# Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7c34d4b3-f443-42cf-b22a-4132ae67f059",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the tokenizer (convert our text to sequence of tokens)\n",
    "tokenizer = BertTokenizerFast.from_pretrained(model_name, do_lower_case=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6fab4359-54a8-4302-a813-555db5918cc3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at /Users/tomaxberg/.cache/huggingface/datasets/ag_news/default/0.0.0/bc2bcb40336ace1a0374767fc29bb0296cdaf8a6da7298436239c54d79180548/cache-dec63c70e6e50560.arrow\n",
      "Loading cached processed dataset at /Users/tomaxberg/.cache/huggingface/datasets/ag_news/default/0.0.0/bc2bcb40336ace1a0374767fc29bb0296cdaf8a6da7298436239c54d79180548/cache-f71c81a33e474491.arrow\n",
      "Loading cached processed dataset at /Users/tomaxberg/.cache/huggingface/datasets/ag_news/default/0.0.0/bc2bcb40336ace1a0374767fc29bb0296cdaf8a6da7298436239c54d79180548/cache-f97efe5561747d9d.arrow\n"
     ]
    }
   ],
   "source": [
    "# tokenize the dataset, truncate when passed 'max_length' and pad with 0's when less than 'max_length'\n",
    "train_tokenized = train_dataset.map(lambda x: tokenizer(x['text'], truncation=True, padding='max_length'), batched=True)\n",
    "val_tokenized = val_dataset.map(lambda x: tokenizer(x['text'], truncation=True, padding='max_length'), batched=True)\n",
    "test_tokenized = test_dataset.map(lambda x: tokenizer(x['text'], truncation=True, padding='max_length'), batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9d87a331-3221-484b-937c-a28f18f08047",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['text', 'label', 'input_ids', 'token_type_ids', 'attention_mask'],\n",
       "    num_rows: 108000\n",
       "})"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_tokenized"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5bbcaae-5195-4511-90aa-65b714c74b52",
   "metadata": {},
   "source": [
    "# Prepare dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "500be08c-7de0-49e8-8cfe-a6148fb8c7b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metal device set to: AMD Radeon Pro 560\n",
      "\n",
      "systemMemory: 16.00 GB\n",
      "maxCacheSize: 2.00 GB\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-05-15 13:24:49.799448: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE4.2 AVX AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-05-15 13:24:49.802553: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:305] Could not identify NUMA node of platform GPU ID 0, defaulting to 0. Your kernel may not have been built with NUMA support.\n",
      "2022-05-15 13:24:49.803392: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:271] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 0 MB memory) -> physical PluggableDevice (device: 0, name: METAL, pci bus id: <undefined>)\n"
     ]
    }
   ],
   "source": [
    "# get data in standard tf.data.Dataset and remove 'text' label as it is not longer needed\n",
    "tf_train_dataset = train_tokenized.remove_columns(['text']).with_format('tensorflow')\n",
    "tf_val_dataset = val_tokenized.remove_columns(['text']).with_format('tensorflow')\n",
    "tf_test_dataset = test_tokenized.remove_columns(['text']).with_format('tensorflow')\n",
    "\n",
    "# convert to tensors\n",
    "train_features = {x: tf_train_dataset[x] for x in tokenizer.model_input_names}\n",
    "train_tf_dataset = tf.data.Dataset.from_tensor_slices((train_features, tf_train_dataset[\"label\"]))\n",
    "train_tf_dataset = train_tf_dataset.shuffle(len(tf_train_dataset)).batch(8)\n",
    "\n",
    "val_features = {x: tf_val_dataset[x] for x in tokenizer.model_input_names}\n",
    "val_tf_dataset = tf.data.Dataset.from_tensor_slices((val_features, tf_val_dataset[\"label\"]))\n",
    "val_tf_dataset = val_tf_dataset.shuffle(len(val_tf_dataset)).batch(8)\n",
    "\n",
    "test_features = {x: tf_test_dataset[x] for x in tokenizer.model_input_names}\n",
    "test_tf_dataset = tf.data.Dataset.from_tensor_slices((test_features, tf_test_dataset[\"label\"]))\n",
    "test_tf_dataset = test_tf_dataset.shuffle(len(test_tf_dataset)).batch(8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "bf7922f8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(512,), dtype=int64, numpy=\n",
       "array([  101,  8130,  2115,  3819,  7473,  2072,  4671,  7473,  2126,\n",
       "        2067,  1999,  2238,  1045,  4081,  2017,  2907,  2125,  2006,\n",
       "        9343,  1037,  2047,  7473,  2127,  3001,  2007,  7473,  2072,\n",
       "        4671, 12057,  1012,  1996,  2047,  2974,  2038,  1996,  4022,\n",
       "        2000, 12099,  5335,  2836,  2138,  2009, 20736,  1996, 26202,\n",
       "        2100,  2214,   102,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0])>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_features['input_ids'][3426]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "165b251f-1e99-4817-b13f-601e44920288",
   "metadata": {},
   "source": [
    "# Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "62df12b4-4b76-42a4-ae52-63e773a61f16",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All model checkpoint layers were used when initializing TFBertForSequenceClassification.\n",
      "\n",
      "Some layers of TFBertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# load the model (pre-trained weights)\n",
    "model = TFAutoModelForSequenceClassification.from_pretrained(model_name, num_labels=num_targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "df4a64e4-eb36-4aa3-a455-69706d83a21a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-04-22 13:40:19.051140: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:113] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "ename": "Error",
     "evalue": "Canceled future for execute_request message before replies were done",
     "output_type": "error",
     "traceback": [
      "Error: Canceled future for execute_request message before replies were done",
      "at t.KernelShellFutureHandler.dispose (/Users/tomaxberg/.vscode/extensions/ms-toolsai.jupyter-2022.3.1000901801/out/extension.js:2:1204175)",
      "at /Users/tomaxberg/.vscode/extensions/ms-toolsai.jupyter-2022.3.1000901801/out/extension.js:2:1223227",
      "at Map.forEach (<anonymous>)",
      "at v._clearKernelState (/Users/tomaxberg/.vscode/extensions/ms-toolsai.jupyter-2022.3.1000901801/out/extension.js:2:1223212)",
      "at v.dispose (/Users/tomaxberg/.vscode/extensions/ms-toolsai.jupyter-2022.3.1000901801/out/extension.js:2:1216694)",
      "at /Users/tomaxberg/.vscode/extensions/ms-toolsai.jupyter-2022.3.1000901801/out/extension.js:2:533674",
      "at t.swallowExceptions (/Users/tomaxberg/.vscode/extensions/ms-toolsai.jupyter-2022.3.1000901801/out/extension.js:2:913059)",
      "at dispose (/Users/tomaxberg/.vscode/extensions/ms-toolsai.jupyter-2022.3.1000901801/out/extension.js:2:533652)",
      "at t.RawSession.dispose (/Users/tomaxberg/.vscode/extensions/ms-toolsai.jupyter-2022.3.1000901801/out/extension.js:2:537330)",
      "at runMicrotasks (<anonymous>)",
      "at processTicksAndRejections (node:internal/process/task_queues:96:5)"
     ]
    }
   ],
   "source": [
    "model.compile(\n",
    "    optimizer=tf.keras.optimizers.Adam(learning_rate=5e-5),\n",
    "    loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "    metrics=tf.metrics.SparseCategoricalAccuracy()\n",
    ")\n",
    "\n",
    "model.fit(train_tf_dataset, validation_data=val_tf_dataset, epochs=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "eb5e3761-c734-4486-9f07-742fecd3e3da",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_pretrained(f\"../models/{model_name}-trained\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0a35504-d973-4685-a6c9-99e5ba4d277c",
   "metadata": {},
   "source": [
    "# Load trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5ad9e29f-e3ad-420b-a147-078e96d25edd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All model checkpoint layers were used when initializing TFBertForSequenceClassification.\n",
      "\n",
      "Some layers of TFBertForSequenceClassification were not initialized from the model checkpoint at bert-large-uncased and are newly initialized: ['classifier']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model = TFAutoModelForSequenceClassification.from_pretrained(model_name, num_labels=num_targets)\n",
    "model.load_weights(f\"../models/{model_name}-trained/tf_model.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f5317a85",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"tf_bert_for_sequence_classification\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " bert (TFBertMainLayer)      multiple                  335141888 \n",
      "                                                                 \n",
      " dropout_73 (Dropout)        multiple                  0         \n",
      "                                                                 \n",
      " classifier (Dense)          multiple                  4100      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 335,145,988\n",
      "Trainable params: 335,145,988\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2db9a544-91e5-4776-b7c4-25d88b652068",
   "metadata": {},
   "source": [
    "## Test the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "e35cfec1-8a07-4b9a-8475-a4cba6212df1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The predicted label is: 2\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<BatchDataset shapes: ({input_ids: (None, 512), token_type_ids: (None, 512), attention_mask: (None, 512)}, (None,)), types: ({input_ids: tf.int64, token_type_ids: tf.int64, attention_mask: tf.int64}, tf.int64)>"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_text = \"Fears for T N pension after talks Unions representing workers at Turner Newall say they are 'disappointed' after talks with stricken parent firm Federal Mogul.\"\n",
    "# input_text = test_tf_dataset\n",
    "input_text_tokenized = tokenizer.encode(input_text,\n",
    "                                        truncation=True,\n",
    "                                        padding=True,\n",
    "                                        return_tensors=\"tf\")\n",
    "prediction = model(input_text_tokenized)\n",
    "prediction_logits = prediction[0]\n",
    "prediction_probs = tf.nn.softmax(prediction_logits,axis=1).numpy()\n",
    "print(f'The predicted label is: {np.argmax(prediction_probs)}')\n",
    "test_tf_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "adccf126-1739-4d78-9c81-312cad9cac80",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'TFBertForSequenceClassification' object has no attribute 'to'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m/Users/tomaxberg/Documents/Learning/KTH/IA150X - KEX/Code/nlp-energy-impact-main/notebooks/Fine-tuning-BERT.ipynb Cell 28'\u001b[0m in \u001b[0;36m<cell line: 5>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/tomaxberg/Documents/Learning/KTH/IA150X%20-%20KEX/Code/nlp-energy-impact-main/notebooks/Fine-tuning-BERT.ipynb#ch0000027?line=2'>3</a>\u001b[0m data_collator \u001b[39m=\u001b[39m DataCollatorWithPadding(tokenizer\u001b[39m=\u001b[39mtokenizer)\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/tomaxberg/Documents/Learning/KTH/IA150X%20-%20KEX/Code/nlp-energy-impact-main/notebooks/Fine-tuning-BERT.ipynb#ch0000027?line=3'>4</a>\u001b[0m training_args \u001b[39m=\u001b[39m TrainingArguments(\u001b[39m\"\u001b[39m\u001b[39mtest-trainer\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/tomaxberg/Documents/Learning/KTH/IA150X%20-%20KEX/Code/nlp-energy-impact-main/notebooks/Fine-tuning-BERT.ipynb#ch0000027?line=4'>5</a>\u001b[0m trainer \u001b[39m=\u001b[39m Trainer(\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/tomaxberg/Documents/Learning/KTH/IA150X%20-%20KEX/Code/nlp-energy-impact-main/notebooks/Fine-tuning-BERT.ipynb#ch0000027?line=5'>6</a>\u001b[0m     model,\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/tomaxberg/Documents/Learning/KTH/IA150X%20-%20KEX/Code/nlp-energy-impact-main/notebooks/Fine-tuning-BERT.ipynb#ch0000027?line=6'>7</a>\u001b[0m     training_args,\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/tomaxberg/Documents/Learning/KTH/IA150X%20-%20KEX/Code/nlp-energy-impact-main/notebooks/Fine-tuning-BERT.ipynb#ch0000027?line=7'>8</a>\u001b[0m     train_dataset\u001b[39m=\u001b[39;49mtrain_dataset,\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/tomaxberg/Documents/Learning/KTH/IA150X%20-%20KEX/Code/nlp-energy-impact-main/notebooks/Fine-tuning-BERT.ipynb#ch0000027?line=8'>9</a>\u001b[0m     eval_dataset\u001b[39m=\u001b[39;49mval_tf_dataset,\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/tomaxberg/Documents/Learning/KTH/IA150X%20-%20KEX/Code/nlp-energy-impact-main/notebooks/Fine-tuning-BERT.ipynb#ch0000027?line=9'>10</a>\u001b[0m     data_collator\u001b[39m=\u001b[39;49mdata_collator,\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/tomaxberg/Documents/Learning/KTH/IA150X%20-%20KEX/Code/nlp-energy-impact-main/notebooks/Fine-tuning-BERT.ipynb#ch0000027?line=10'>11</a>\u001b[0m     tokenizer\u001b[39m=\u001b[39;49mtokenizer,\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/tomaxberg/Documents/Learning/KTH/IA150X%20-%20KEX/Code/nlp-energy-impact-main/notebooks/Fine-tuning-BERT.ipynb#ch0000027?line=11'>12</a>\u001b[0m )\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/nlp_energy_project/lib/python3.8/site-packages/transformers/trainer.py:382\u001b[0m, in \u001b[0;36mTrainer.__init__\u001b[0;34m(self, model, args, data_collator, train_dataset, eval_dataset, tokenizer, model_init, compute_metrics, callbacks, optimizers, preprocess_logits_for_metrics)\u001b[0m\n\u001b[1;32m    <a href='file:///Users/tomaxberg/opt/anaconda3/envs/nlp_energy_project/lib/python3.8/site-packages/transformers/trainer.py?line=378'>379</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtokenizer \u001b[39m=\u001b[39m tokenizer\n\u001b[1;32m    <a href='file:///Users/tomaxberg/opt/anaconda3/envs/nlp_energy_project/lib/python3.8/site-packages/transformers/trainer.py?line=380'>381</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mplace_model_on_device:\n\u001b[0;32m--> <a href='file:///Users/tomaxberg/opt/anaconda3/envs/nlp_energy_project/lib/python3.8/site-packages/transformers/trainer.py?line=381'>382</a>\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_move_model_to_device(model, args\u001b[39m.\u001b[39;49mdevice)\n\u001b[1;32m    <a href='file:///Users/tomaxberg/opt/anaconda3/envs/nlp_energy_project/lib/python3.8/site-packages/transformers/trainer.py?line=383'>384</a>\u001b[0m \u001b[39m# Force n_gpu to 1 to avoid DataParallel as MP will manage the GPUs\u001b[39;00m\n\u001b[1;32m    <a href='file:///Users/tomaxberg/opt/anaconda3/envs/nlp_energy_project/lib/python3.8/site-packages/transformers/trainer.py?line=384'>385</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mis_model_parallel:\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/nlp_energy_project/lib/python3.8/site-packages/transformers/trainer.py:548\u001b[0m, in \u001b[0;36mTrainer._move_model_to_device\u001b[0;34m(self, model, device)\u001b[0m\n\u001b[1;32m    <a href='file:///Users/tomaxberg/opt/anaconda3/envs/nlp_energy_project/lib/python3.8/site-packages/transformers/trainer.py?line=546'>547</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_move_model_to_device\u001b[39m(\u001b[39mself\u001b[39m, model, device):\n\u001b[0;32m--> <a href='file:///Users/tomaxberg/opt/anaconda3/envs/nlp_energy_project/lib/python3.8/site-packages/transformers/trainer.py?line=547'>548</a>\u001b[0m     model \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39;49mto(device)\n\u001b[1;32m    <a href='file:///Users/tomaxberg/opt/anaconda3/envs/nlp_energy_project/lib/python3.8/site-packages/transformers/trainer.py?line=548'>549</a>\u001b[0m     \u001b[39m# Moving a model to an XLA device disconnects the tied weights, so we have to retie them.\u001b[39;00m\n\u001b[1;32m    <a href='file:///Users/tomaxberg/opt/anaconda3/envs/nlp_energy_project/lib/python3.8/site-packages/transformers/trainer.py?line=549'>550</a>\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39margs\u001b[39m.\u001b[39mparallel_mode \u001b[39m==\u001b[39m ParallelMode\u001b[39m.\u001b[39mTPU \u001b[39mand\u001b[39;00m \u001b[39mhasattr\u001b[39m(model, \u001b[39m\"\u001b[39m\u001b[39mtie_weights\u001b[39m\u001b[39m\"\u001b[39m):\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'TFBertForSequenceClassification' object has no attribute 'to'"
     ]
    }
   ],
   "source": [
    "from transformers import TrainingArguments\n",
    "\n",
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n",
    "training_args = TrainingArguments(\"test-trainer\")\n",
    "trainer = Trainer(\n",
    "    model,\n",
    "    training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=val_tf_dataset,\n",
    "    data_collator=data_collator,\n",
    "    tokenizer=tokenizer,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "426b4666",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'NoneType' object is not iterable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/Users/tomaxberg/Documents/Learning/KTH/IA150X - KEX/Code/nlp-energy-impact-main/notebooks/Fine-tuning-BERT.ipynb Cell 29'\u001b[0m in \u001b[0;36m<cell line: 19>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/tomaxberg/Documents/Learning/KTH/IA150X%20-%20KEX/Code/nlp-energy-impact-main/notebooks/Fine-tuning-BERT.ipynb#ch0000028?line=13'>14</a>\u001b[0m         flops \u001b[39m=\u001b[39m tf\u001b[39m.\u001b[39mcompat\u001b[39m.\u001b[39mv1\u001b[39m.\u001b[39mprofiler\u001b[39m.\u001b[39mprofile(\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/tomaxberg/Documents/Learning/KTH/IA150X%20-%20KEX/Code/nlp-energy-impact-main/notebooks/Fine-tuning-BERT.ipynb#ch0000028?line=14'>15</a>\u001b[0m             graph\u001b[39m=\u001b[39mgraph, run_meta\u001b[39m=\u001b[39mrun_meta, cmd\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mop\u001b[39m\u001b[39m\"\u001b[39m, options\u001b[39m=\u001b[39mopts)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/tomaxberg/Documents/Learning/KTH/IA150X%20-%20KEX/Code/nlp-energy-impact-main/notebooks/Fine-tuning-BERT.ipynb#ch0000028?line=15'>16</a>\u001b[0m         \u001b[39mreturn\u001b[39;00m flops\u001b[39m.\u001b[39mtotal_float_ops\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/tomaxberg/Documents/Learning/KTH/IA150X%20-%20KEX/Code/nlp-energy-impact-main/notebooks/Fine-tuning-BERT.ipynb#ch0000028?line=18'>19</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mGFLOPS: \u001b[39m\u001b[39m{\u001b[39;00mget_flops(model) \u001b[39m/\u001b[39m \u001b[39m10\u001b[39m \u001b[39m*\u001b[39m\u001b[39m*\u001b[39m \u001b[39m9\u001b[39m\u001b[39m:\u001b[39;00m\u001b[39m.03\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m \u001b[39m\u001b[39m\"\u001b[39m)\n",
      "\u001b[1;32m/Users/tomaxberg/Documents/Learning/KTH/IA150X - KEX/Code/nlp-energy-impact-main/notebooks/Fine-tuning-BERT.ipynb Cell 29'\u001b[0m in \u001b[0;36mget_flops\u001b[0;34m(model)\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/tomaxberg/Documents/Learning/KTH/IA150X%20-%20KEX/Code/nlp-energy-impact-main/notebooks/Fine-tuning-BERT.ipynb#ch0000028?line=3'>4</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mget_flops\u001b[39m(model):\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/tomaxberg/Documents/Learning/KTH/IA150X%20-%20KEX/Code/nlp-energy-impact-main/notebooks/Fine-tuning-BERT.ipynb#ch0000028?line=4'>5</a>\u001b[0m     concrete \u001b[39m=\u001b[39m tf\u001b[39m.\u001b[39mfunction(\u001b[39mlambda\u001b[39;00m inputs: model(inputs))\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/tomaxberg/Documents/Learning/KTH/IA150X%20-%20KEX/Code/nlp-energy-impact-main/notebooks/Fine-tuning-BERT.ipynb#ch0000028?line=5'>6</a>\u001b[0m     concrete_func \u001b[39m=\u001b[39m concrete\u001b[39m.\u001b[39mget_concrete_function(\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/tomaxberg/Documents/Learning/KTH/IA150X%20-%20KEX/Code/nlp-energy-impact-main/notebooks/Fine-tuning-BERT.ipynb#ch0000028?line=6'>7</a>\u001b[0m         [tf\u001b[39m.\u001b[39mTensorSpec([\u001b[39m1\u001b[39m, \u001b[39m*\u001b[39minputs\u001b[39m.\u001b[39mshape[\u001b[39m1\u001b[39m:]]) \u001b[39mfor\u001b[39;00m inputs \u001b[39min\u001b[39;00m model\u001b[39m.\u001b[39minputs])\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/tomaxberg/Documents/Learning/KTH/IA150X%20-%20KEX/Code/nlp-energy-impact-main/notebooks/Fine-tuning-BERT.ipynb#ch0000028?line=7'>8</a>\u001b[0m     frozen_func, graph_def \u001b[39m=\u001b[39m convert_variables_to_constants_v2_as_graph(\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/tomaxberg/Documents/Learning/KTH/IA150X%20-%20KEX/Code/nlp-energy-impact-main/notebooks/Fine-tuning-BERT.ipynb#ch0000028?line=8'>9</a>\u001b[0m         concrete_func)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/tomaxberg/Documents/Learning/KTH/IA150X%20-%20KEX/Code/nlp-energy-impact-main/notebooks/Fine-tuning-BERT.ipynb#ch0000028?line=9'>10</a>\u001b[0m     \u001b[39mwith\u001b[39;00m tf\u001b[39m.\u001b[39mGraph()\u001b[39m.\u001b[39mas_default() \u001b[39mas\u001b[39;00m graph:\n",
      "\u001b[0;31mTypeError\u001b[0m: 'NoneType' object is not iterable"
     ]
    }
   ],
   "source": [
    "from tensorflow.python.framework.convert_to_constants import convert_variables_to_constants_v2_as_graph\n",
    "\n",
    "\n",
    "def get_flops(model):\n",
    "    concrete = tf.function(lambda inputs: model(inputs))\n",
    "    concrete_func = concrete.get_concrete_function(\n",
    "        [tf.TensorSpec([1, *inputs.shape[1:]]) for inputs in model.inputs])\n",
    "    frozen_func, graph_def = convert_variables_to_constants_v2_as_graph(\n",
    "        concrete_func)\n",
    "    with tf.Graph().as_default() as graph:\n",
    "        tf.graph_util.import_graph_def(graph_def, name='')\n",
    "        run_meta = tf.compat.v1.RunMetadata()\n",
    "        opts = tf.compat.v1.profiler.ProfileOptionBuilder.float_operation()\n",
    "        flops = tf.compat.v1.profiler.profile(\n",
    "            graph=graph, run_meta=run_meta, cmd=\"op\", options=opts)\n",
    "        return flops.total_float_ops\n",
    "\n",
    "\n",
    "print(f\"GFLOPS: {get_flops(model) / 10 ** 9:.06} \")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "54559f56",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'profile' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/jg/32b9xm0j48x73b3pq7g776zh0000gn/T/ipykernel_13405/794876664.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mflops\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprofile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mflops\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'profile' is not defined"
     ]
    }
   ],
   "source": [
    "flops, params = profile(model, inputs, verbose=False)\n",
    "flops "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fe9a581",
   "metadata": {},
   "source": [
    "### Roc and stuff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a54e97d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: Retrying (Retry(total=4, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<pip._vendor.urllib3.connection.HTTPSConnection object at 0x7fa667607700>: Failed to establish a new connection: [Errno 8] nodename nor servname provided, or not known')': /simple/sklearn/\u001b[0m\n",
      "\u001b[33mWARNING: Retrying (Retry(total=3, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<pip._vendor.urllib3.connection.HTTPSConnection object at 0x7fa6676074f0>: Failed to establish a new connection: [Errno 8] nodename nor servname provided, or not known')': /simple/sklearn/\u001b[0m\n",
      "\u001b[33mWARNING: Retrying (Retry(total=2, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<pip._vendor.urllib3.connection.HTTPSConnection object at 0x7fa667618160>: Failed to establish a new connection: [Errno 8] nodename nor servname provided, or not known')': /simple/sklearn/\u001b[0m\n",
      "\u001b[33mWARNING: Retrying (Retry(total=1, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<pip._vendor.urllib3.connection.HTTPSConnection object at 0x7fa667618400>: Failed to establish a new connection: [Errno 8] nodename nor servname provided, or not known')': /simple/sklearn/\u001b[0m\n",
      "\u001b[33mWARNING: Retrying (Retry(total=0, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<pip._vendor.urllib3.connection.HTTPSConnection object at 0x7fa6676185e0>: Failed to establish a new connection: [Errno 8] nodename nor servname provided, or not known')': /simple/sklearn/\u001b[0m\n",
      "\u001b[31mERROR: Could not find a version that satisfies the requirement sklearn (from versions: none)\u001b[0m\n",
      "\u001b[31mERROR: No matching distribution found for sklearn\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install sklearn seaborn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e2197887",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'sklearn'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m/Users/tomaxberg/Documents/Learning/KTH/IA150X - KEX/Code/nlp-energy-impact-main/notebooks/Fine-tuning-BERT.ipynb Cell 33'\u001b[0m in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/tomaxberg/Documents/Learning/KTH/IA150X%20-%20KEX/Code/nlp-energy-impact-main/notebooks/Fine-tuning-BERT.ipynb#ch0000030?line=0'>1</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtqdm\u001b[39;00m \u001b[39mimport\u001b[39;00m tqdm\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/tomaxberg/Documents/Learning/KTH/IA150X%20-%20KEX/Code/nlp-energy-impact-main/notebooks/Fine-tuning-BERT.ipynb#ch0000030?line=1'>2</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtqdm\u001b[39;00m \u001b[39mimport\u001b[39;00m tqdm_notebook\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/tomaxberg/Documents/Learning/KTH/IA150X%20-%20KEX/Code/nlp-energy-impact-main/notebooks/Fine-tuning-BERT.ipynb#ch0000030?line=2'>3</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39msklearn\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mmetrics\u001b[39;00m \u001b[39mimport\u001b[39;00m auc\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/tomaxberg/Documents/Learning/KTH/IA150X%20-%20KEX/Code/nlp-energy-impact-main/notebooks/Fine-tuning-BERT.ipynb#ch0000030?line=3'>4</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39msklearn\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mmetrics\u001b[39;00m \u001b[39mimport\u001b[39;00m classification_report\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/tomaxberg/Documents/Learning/KTH/IA150X%20-%20KEX/Code/nlp-energy-impact-main/notebooks/Fine-tuning-BERT.ipynb#ch0000030?line=4'>5</a>\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mseaborn\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39msns\u001b[39;00m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'sklearn'"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "from tqdm import tqdm_notebook\n",
    "from sklearn.metrics import auc\n",
    "from sklearn.metrics import classification_report\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "mpl.rcParams['figure.figsize'] = (12, 10)\n",
    "colors = plt.rcParams['axes.prop_cycle'].by_key()['color']\n",
    "\n",
    "def plot_loss(history):\n",
    "# Use a log scale to show the wide range of values.\n",
    "    plt.semilogy(history.epoch,  history.history['loss'],\n",
    "               color='red', label='Train Loss')\n",
    "    plt.semilogy(history.epoch,  history.history['val_loss'],\n",
    "          color='green', label='Val Loss',\n",
    "          linestyle=\"--\")\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "  \n",
    "    plt.legend()\n",
    "    \n",
    "    \n",
    "def plot_metrics(history):\n",
    "    metrics =  ['loss', 'auc', 'precision', 'recall']\n",
    "    for n, metric in enumerate(metrics):\n",
    "        name = metric.replace(\"_\",\" \").capitalize()\n",
    "        plt.subplot(2,2,n+1)\n",
    "        plt.plot(history.epoch,  history.history[metric], color=colors[0], label='Train')\n",
    "        plt.plot(history.epoch, history.history['val_'+metric],\n",
    "                 color=colors[0], linestyle=\"--\", label='Val')\n",
    "        plt.xlabel('Epoch')\n",
    "        plt.ylabel(name)\n",
    "        if metric == 'loss':\n",
    "            plt.ylim([0, plt.ylim()[1]])\n",
    "        elif metric == 'auc':\n",
    "            plt.ylim([0.8,1])\n",
    "        else:\n",
    "            plt.ylim([0,1])\n",
    "\n",
    "        plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5a43aa4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_cm(y_true, y_pred, title):\n",
    "    ''''\n",
    "    input y_true-Ground Truth Labels\n",
    "          y_pred-Predicted Value of Model\n",
    "          title-What Title to give to the confusion matrix\n",
    "    \n",
    "    Draws a Confusion Matrix for better understanding of how the model is working\n",
    "    \n",
    "    return None\n",
    "    \n",
    "    '''\n",
    "    \n",
    "    figsize=(10,10)\n",
    "    cm = confusion_matrix(y_true, y_pred, labels=np.unique(y_true))\n",
    "    cm_sum = np.sum(cm, axis=1, keepdims=True)\n",
    "    cm_perc = cm / cm_sum.astype(float) * 100\n",
    "    annot = np.empty_like(cm).astype(str)\n",
    "    nrows, ncols = cm.shape\n",
    "    for i in range(nrows):\n",
    "        for j in range(ncols):\n",
    "            c = cm[i, j]\n",
    "            p = cm_perc[i, j]\n",
    "            if i == j:\n",
    "                s = cm_sum[i]\n",
    "                annot[i, j] = '%.1f%%\\n%d/%d' % (p, c, s)\n",
    "            elif c == 0:\n",
    "                annot[i, j] = ''\n",
    "            else:\n",
    "                annot[i, j] = '%.1f%%\\n%d' % (p, c)\n",
    "    cm = pd.DataFrame(cm, index=np.unique(y_true), columns=np.unique(y_true))\n",
    "    cm.index.name = 'Actual'\n",
    "    cm.columns.name = 'Predicted'\n",
    "    fig, ax = plt.subplots(figsize=figsize)\n",
    "    plt.title(title)\n",
    "    sns.heatmap(cm, cmap= \"YlGnBu\", annot=annot, fmt='', ax=ax)\n",
    "\n",
    "def roc_curve_plot(fpr,tpr,roc_auc):\n",
    "    plt.figure()\n",
    "    lw = 2\n",
    "    plt.plot(fpr, tpr, color='darkorange',\n",
    "             lw=lw, label='ROC curve (area = %0.2f)' %roc_auc)\n",
    "    plt.plot([0, 1], [0, 1], color='navy', lw=lw, linestyle='--')\n",
    "    plt.xlim([0.0, 1.0])\n",
    "    plt.ylim([0.0, 1.05])\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.title('Receiver operating characteristic example')\n",
    "    plt.legend(loc=\"lower right\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "17099d09",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'valid' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/Users/tomaxberg/Documents/Learning/KTH/IA150X - KEX/Code/nlp-energy-impact-main/notebooks/Fine-tuning-BERT.ipynb Cell 36'\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/tomaxberg/Documents/Learning/KTH/IA150X%20-%20KEX/Code/nlp-energy-impact-main/notebooks/Fine-tuning-BERT.ipynb#ch0000035?line=0'>1</a>\u001b[0m y_predict\u001b[39m=\u001b[39mmodel\u001b[39m.\u001b[39mpredict(valid, verbose\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m)\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/tomaxberg/Documents/Learning/KTH/IA150X%20-%20KEX/Code/nlp-energy-impact-main/notebooks/Fine-tuning-BERT.ipynb#ch0000035?line=1'>2</a>\u001b[0m y_predict[ y_predict\u001b[39m>\u001b[39m \u001b[39m0.5\u001b[39m] \u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/tomaxberg/Documents/Learning/KTH/IA150X%20-%20KEX/Code/nlp-energy-impact-main/notebooks/Fine-tuning-BERT.ipynb#ch0000035?line=2'>3</a>\u001b[0m y_predict[y_predict \u001b[39m<\u001b[39m\u001b[39m=\u001b[39m \u001b[39m0.5\u001b[39m] \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'valid' is not defined"
     ]
    }
   ],
   "source": [
    "y_predict=model.predict(valid, verbose=1)\n",
    "y_predict[ y_predict> 0.5] = 1\n",
    "y_predict[y_predict <= 0.5] = 0\n",
    "plot_cm(y_valid, y_predict, 'Distil BERT Performance-Confusion Matrix')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11ee5141",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_predict_prob=model.predict(valid, verbose=1)\n",
    "fpr, tpr, _ = roc_curve(y_valid,y_predict_prob)\n",
    "roc_auc = auc(fpr, tpr)\n",
    "roc_curve_plot(fpr,tpr,roc_auc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "dcf0d47b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-05-15 13:25:15.971702: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:113] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "686/950 [====================>.........] - ETA: 25:19 - loss: 1.3925 - sparse_categorical_accuracy: 0.2526"
     ]
    }
   ],
   "source": [
    "model.compile(\n",
    "    optimizer=tf.keras.optimizers.Adam(learning_rate=5e-5),\n",
    "    loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "    metrics=tf.metrics.SparseCategoricalAccuracy()\n",
    ")\n",
    "result = model.evaluate(test_tf_dataset)\n",
    "print(\"test loss, test acc:\", result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed8b06f6",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "You must compile your model before training/testing. Use `model.compile(optimizer, loss)`.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m/Users/tomaxberg/Documents/Learning/KTH/IA150X - KEX/Code/nlp-energy-impact-main/notebooks/Fine-tuning-BERT.ipynb Cell 40'\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/tomaxberg/Documents/Learning/KTH/IA150X%20-%20KEX/Code/nlp-energy-impact-main/notebooks/Fine-tuning-BERT.ipynb#ch0000039?line=0'>1</a>\u001b[0m result \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39;49mevaluate(test_tf_dataset)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/tf-metal/lib/python3.8/site-packages/keras/utils/traceback_utils.py:67\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     <a href='file:///Users/tomaxberg/opt/anaconda3/envs/tf-metal/lib/python3.8/site-packages/keras/utils/traceback_utils.py?line=64'>65</a>\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:  \u001b[39m# pylint: disable=broad-except\u001b[39;00m\n\u001b[1;32m     <a href='file:///Users/tomaxberg/opt/anaconda3/envs/tf-metal/lib/python3.8/site-packages/keras/utils/traceback_utils.py?line=65'>66</a>\u001b[0m   filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n\u001b[0;32m---> <a href='file:///Users/tomaxberg/opt/anaconda3/envs/tf-metal/lib/python3.8/site-packages/keras/utils/traceback_utils.py?line=66'>67</a>\u001b[0m   \u001b[39mraise\u001b[39;00m e\u001b[39m.\u001b[39mwith_traceback(filtered_tb) \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39m\n\u001b[1;32m     <a href='file:///Users/tomaxberg/opt/anaconda3/envs/tf-metal/lib/python3.8/site-packages/keras/utils/traceback_utils.py?line=67'>68</a>\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[1;32m     <a href='file:///Users/tomaxberg/opt/anaconda3/envs/tf-metal/lib/python3.8/site-packages/keras/utils/traceback_utils.py?line=68'>69</a>\u001b[0m   \u001b[39mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/tf-metal/lib/python3.8/site-packages/keras/engine/training.py:3059\u001b[0m, in \u001b[0;36mModel._assert_compile_was_called\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   <a href='file:///Users/tomaxberg/opt/anaconda3/envs/tf-metal/lib/python3.8/site-packages/keras/engine/training.py?line=3052'>3053</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_assert_compile_was_called\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[1;32m   <a href='file:///Users/tomaxberg/opt/anaconda3/envs/tf-metal/lib/python3.8/site-packages/keras/engine/training.py?line=3053'>3054</a>\u001b[0m   \u001b[39m# Checks whether `compile` has been called. If it has been called,\u001b[39;00m\n\u001b[1;32m   <a href='file:///Users/tomaxberg/opt/anaconda3/envs/tf-metal/lib/python3.8/site-packages/keras/engine/training.py?line=3054'>3055</a>\u001b[0m   \u001b[39m# then the optimizer is set. This is different from whether the\u001b[39;00m\n\u001b[1;32m   <a href='file:///Users/tomaxberg/opt/anaconda3/envs/tf-metal/lib/python3.8/site-packages/keras/engine/training.py?line=3055'>3056</a>\u001b[0m   \u001b[39m# model is compiled\u001b[39;00m\n\u001b[1;32m   <a href='file:///Users/tomaxberg/opt/anaconda3/envs/tf-metal/lib/python3.8/site-packages/keras/engine/training.py?line=3056'>3057</a>\u001b[0m   \u001b[39m# (i.e. whether the model is built and its inputs/outputs are set).\u001b[39;00m\n\u001b[1;32m   <a href='file:///Users/tomaxberg/opt/anaconda3/envs/tf-metal/lib/python3.8/site-packages/keras/engine/training.py?line=3057'>3058</a>\u001b[0m   \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_is_compiled:\n\u001b[0;32m-> <a href='file:///Users/tomaxberg/opt/anaconda3/envs/tf-metal/lib/python3.8/site-packages/keras/engine/training.py?line=3058'>3059</a>\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(\u001b[39m'\u001b[39m\u001b[39mYou must compile your model before \u001b[39m\u001b[39m'\u001b[39m\n\u001b[1;32m   <a href='file:///Users/tomaxberg/opt/anaconda3/envs/tf-metal/lib/python3.8/site-packages/keras/engine/training.py?line=3059'>3060</a>\u001b[0m                        \u001b[39m'\u001b[39m\u001b[39mtraining/testing. \u001b[39m\u001b[39m'\u001b[39m\n\u001b[1;32m   <a href='file:///Users/tomaxberg/opt/anaconda3/envs/tf-metal/lib/python3.8/site-packages/keras/engine/training.py?line=3060'>3061</a>\u001b[0m                        \u001b[39m'\u001b[39m\u001b[39mUse `model.compile(optimizer, loss)`.\u001b[39m\u001b[39m'\u001b[39m)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: You must compile your model before training/testing. Use `model.compile(optimizer, loss)`."
     ]
    }
   ],
   "source": [
    "result = model.evaluate(test_tf_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "28f38cd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = [0.25786691904067993, 0.9277631044387817]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8addc0e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test loss, test acc: [0.25786691904067993, 0.9277631044387817]\n"
     ]
    }
   ],
   "source": [
    "print(\"test loss, test acc:\", results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "56133302",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Exception encountered when calling layer \"tf_bert_for_sequence_classification\" (type TFBertForSequenceClassification).\n\nData of type <class 'tensorflow.python.data.ops.dataset_ops.BatchDataset'> is not allowed only (<class 'tensorflow.python.framework.ops.Tensor'>, <class 'bool'>, <class 'int'>, <class 'transformers.utils.generic.ModelOutput'>, <class 'tuple'>, <class 'list'>, <class 'dict'>, <class 'numpy.ndarray'>, <class 'tensorflow.python.keras.engine.keras_tensor.KerasTensor'>) is accepted for input_ids.\n\nCall arguments received:\n   input_ids=<BatchDataset element_spec=({'input_ids': TensorSpec(shape=(None, 512), dtype=tf.int64, name=None), 'token_type_ids': TensorSpec(shape=(None, 512), dtype=tf.int64, name=None), 'attention_mask': TensorSpec(shape=(None, 512), dtype=tf.int64, name=None)}, TensorSpec(shape=(None,), dtype=tf.int64, name=None))>\n   attention_mask=None\n   token_type_ids=None\n   position_ids=None\n   head_mask=None\n   inputs_embeds=None\n   output_attentions=None\n   output_hidden_states=None\n   return_dict=None\n   labels=None\n   training=False",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m/Users/tomaxberg/Documents/Learning/KTH/IA150X - KEX/Code/nlp-energy-impact-main/notebooks/Fine-tuning-BERT.ipynb Cell 40'\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/tomaxberg/Documents/Learning/KTH/IA150X%20-%20KEX/Code/nlp-energy-impact-main/notebooks/Fine-tuning-BERT.ipynb#ch0000042?line=0'>1</a>\u001b[0m results \u001b[39m=\u001b[39m model(test_tf_dataset)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/tf-metal/lib/python3.8/site-packages/keras/utils/traceback_utils.py:67\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     <a href='file:///Users/tomaxberg/opt/anaconda3/envs/tf-metal/lib/python3.8/site-packages/keras/utils/traceback_utils.py?line=64'>65</a>\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:  \u001b[39m# pylint: disable=broad-except\u001b[39;00m\n\u001b[1;32m     <a href='file:///Users/tomaxberg/opt/anaconda3/envs/tf-metal/lib/python3.8/site-packages/keras/utils/traceback_utils.py?line=65'>66</a>\u001b[0m   filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n\u001b[0;32m---> <a href='file:///Users/tomaxberg/opt/anaconda3/envs/tf-metal/lib/python3.8/site-packages/keras/utils/traceback_utils.py?line=66'>67</a>\u001b[0m   \u001b[39mraise\u001b[39;00m e\u001b[39m.\u001b[39mwith_traceback(filtered_tb) \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39m\n\u001b[1;32m     <a href='file:///Users/tomaxberg/opt/anaconda3/envs/tf-metal/lib/python3.8/site-packages/keras/utils/traceback_utils.py?line=67'>68</a>\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[1;32m     <a href='file:///Users/tomaxberg/opt/anaconda3/envs/tf-metal/lib/python3.8/site-packages/keras/utils/traceback_utils.py?line=68'>69</a>\u001b[0m   \u001b[39mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/tf-metal/lib/python3.8/site-packages/transformers/modeling_tf_utils.py:382\u001b[0m, in \u001b[0;36munpack_inputs.<locals>.run_call_with_unpacked_inputs\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    <a href='file:///Users/tomaxberg/opt/anaconda3/envs/tf-metal/lib/python3.8/site-packages/transformers/modeling_tf_utils.py?line=379'>380</a>\u001b[0m main_input_name \u001b[39m=\u001b[39m \u001b[39mgetattr\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mmain_input_name\u001b[39m\u001b[39m\"\u001b[39m, func\u001b[39m.\u001b[39m\u001b[39m__code__\u001b[39m\u001b[39m.\u001b[39mco_varnames[\u001b[39m1\u001b[39m])\n\u001b[1;32m    <a href='file:///Users/tomaxberg/opt/anaconda3/envs/tf-metal/lib/python3.8/site-packages/transformers/modeling_tf_utils.py?line=380'>381</a>\u001b[0m main_input \u001b[39m=\u001b[39m fn_args_and_kwargs\u001b[39m.\u001b[39mpop(main_input_name, \u001b[39mNone\u001b[39;00m)\n\u001b[0;32m--> <a href='file:///Users/tomaxberg/opt/anaconda3/envs/tf-metal/lib/python3.8/site-packages/transformers/modeling_tf_utils.py?line=381'>382</a>\u001b[0m unpacked_inputs \u001b[39m=\u001b[39m input_processing(func, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mconfig, main_input, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mfn_args_and_kwargs)\n\u001b[1;32m    <a href='file:///Users/tomaxberg/opt/anaconda3/envs/tf-metal/lib/python3.8/site-packages/transformers/modeling_tf_utils.py?line=382'>383</a>\u001b[0m \u001b[39mreturn\u001b[39;00m func(\u001b[39mself\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39munpacked_inputs)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/tf-metal/lib/python3.8/site-packages/transformers/modeling_tf_utils.py:504\u001b[0m, in \u001b[0;36minput_processing\u001b[0;34m(func, config, input_ids, **kwargs)\u001b[0m\n\u001b[1;32m    <a href='file:///Users/tomaxberg/opt/anaconda3/envs/tf-metal/lib/python3.8/site-packages/transformers/modeling_tf_utils.py?line=501'>502</a>\u001b[0m         output[parameter_names[\u001b[39m0\u001b[39m]] \u001b[39m=\u001b[39m input_ids\n\u001b[1;32m    <a href='file:///Users/tomaxberg/opt/anaconda3/envs/tf-metal/lib/python3.8/site-packages/transformers/modeling_tf_utils.py?line=502'>503</a>\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> <a href='file:///Users/tomaxberg/opt/anaconda3/envs/tf-metal/lib/python3.8/site-packages/transformers/modeling_tf_utils.py?line=503'>504</a>\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m    <a href='file:///Users/tomaxberg/opt/anaconda3/envs/tf-metal/lib/python3.8/site-packages/transformers/modeling_tf_utils.py?line=504'>505</a>\u001b[0m             \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mData of type \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mtype\u001b[39m(input_ids)\u001b[39m}\u001b[39;00m\u001b[39m is not allowed only \u001b[39m\u001b[39m{\u001b[39;00mallowed_types\u001b[39m}\u001b[39;00m\u001b[39m is accepted for \u001b[39m\u001b[39m{\u001b[39;00mparameter_names[\u001b[39m0\u001b[39m]\u001b[39m}\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    <a href='file:///Users/tomaxberg/opt/anaconda3/envs/tf-metal/lib/python3.8/site-packages/transformers/modeling_tf_utils.py?line=505'>506</a>\u001b[0m         )\n\u001b[1;32m    <a href='file:///Users/tomaxberg/opt/anaconda3/envs/tf-metal/lib/python3.8/site-packages/transformers/modeling_tf_utils.py?line=507'>508</a>\u001b[0m \u001b[39m# Populates any unspecified argument with their default value, according to the signature.\u001b[39;00m\n\u001b[1;32m    <a href='file:///Users/tomaxberg/opt/anaconda3/envs/tf-metal/lib/python3.8/site-packages/transformers/modeling_tf_utils.py?line=508'>509</a>\u001b[0m \u001b[39mfor\u001b[39;00m name \u001b[39min\u001b[39;00m parameter_names:\n",
      "\u001b[0;31mValueError\u001b[0m: Exception encountered when calling layer \"tf_bert_for_sequence_classification\" (type TFBertForSequenceClassification).\n\nData of type <class 'tensorflow.python.data.ops.dataset_ops.BatchDataset'> is not allowed only (<class 'tensorflow.python.framework.ops.Tensor'>, <class 'bool'>, <class 'int'>, <class 'transformers.utils.generic.ModelOutput'>, <class 'tuple'>, <class 'list'>, <class 'dict'>, <class 'numpy.ndarray'>, <class 'tensorflow.python.keras.engine.keras_tensor.KerasTensor'>) is accepted for input_ids.\n\nCall arguments received:\n   input_ids=<BatchDataset element_spec=({'input_ids': TensorSpec(shape=(None, 512), dtype=tf.int64, name=None), 'token_type_ids': TensorSpec(shape=(None, 512), dtype=tf.int64, name=None), 'attention_mask': TensorSpec(shape=(None, 512), dtype=tf.int64, name=None)}, TensorSpec(shape=(None,), dtype=tf.int64, name=None))>\n   attention_mask=None\n   token_type_ids=None\n   position_ids=None\n   head_mask=None\n   inputs_embeds=None\n   output_attentions=None\n   output_hidden_states=None\n   return_dict=None\n   labels=None\n   training=False"
     ]
    }
   ],
   "source": [
    "results = model(test_tf_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0868a480",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'sklearn'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m/Users/tomaxberg/Documents/Learning/KTH/IA150X - KEX/Code/nlp-energy-impact-main/notebooks/Fine-tuning-BERT.ipynb Cell 42'\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/tomaxberg/Documents/Learning/KTH/IA150X%20-%20KEX/Code/nlp-energy-impact-main/notebooks/Fine-tuning-BERT.ipynb#ch0000041?line=0'>1</a>\u001b[0m \u001b[39m# from sklearn.datasets import load_breast_cancer\u001b[39;00m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/tomaxberg/Documents/Learning/KTH/IA150X%20-%20KEX/Code/nlp-energy-impact-main/notebooks/Fine-tuning-BERT.ipynb#ch0000041?line=1'>2</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39msklearn\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mlinear_model\u001b[39;00m \u001b[39mimport\u001b[39;00m LogisticRegression\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/tomaxberg/Documents/Learning/KTH/IA150X%20-%20KEX/Code/nlp-energy-impact-main/notebooks/Fine-tuning-BERT.ipynb#ch0000041?line=2'>3</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39msklearn\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mdatasets\u001b[39;00m \u001b[39mimport\u001b[39;00m load_iris\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/tomaxberg/Documents/Learning/KTH/IA150X%20-%20KEX/Code/nlp-energy-impact-main/notebooks/Fine-tuning-BERT.ipynb#ch0000041?line=3'>4</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39msklearn\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mmetrics\u001b[39;00m \u001b[39mimport\u001b[39;00m roc_auc_score\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'sklearn'"
     ]
    }
   ],
   "source": [
    "# from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "X, y = test_features['input_ids'].numpy(), results\n",
    "# X, y = load_iris(return_X_y=True)\n",
    "clf = LogisticRegression(solver=\"liblinear\").fit(X, y)\n",
    "roc_auc_score(y, clf.predict_proba(X), multi_class='ovr')\n",
    "\n",
    "test_dataset['label'] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "b5b1d4fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_text = \"Fears for T N pension after talks Unions representing workers at Turner Newall say they are 'disappointed' after talks with stricken parent firm Federal Mogul.\"\n",
    "# input_text = test_tf_dataset\n",
    "input_text_tokenized = tokenizer.encode(input_text,\n",
    "                                        truncation=True,\n",
    "                                        padding=True,\n",
    "                                        return_tensors=\"tf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "9c9e20a2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(1, 32), dtype=int32, numpy=\n",
       "array([[  101, 10069,  2005,  1056,  1050, 11550,  2044,  7566,  9209,\n",
       "         5052,  3667,  2012,  6769,  2047,  8095,  2360,  2027,  2024,\n",
       "         1005,  9364,  1005,  2044,  7566,  2007, 16654,  6687,  3813,\n",
       "         2976,  9587, 24848,  1012,   102]], dtype=int32)>"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_text_tokenized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "41c924de",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(512,), dtype=int64, numpy=\n",
       "array([  101,  1996,  2679,  2003,  2006,  1024,  2117,  2797,  2136,\n",
       "        4520,  4888,  3058,  2005,  2529,  2686, 28968,  1006,  2686,\n",
       "        1012,  4012,  1007,  2686,  1012,  4012,  1011,  4361,  1010,\n",
       "        2710,  1011,  1011,  1037,  2117,  1032,  2136,  1997,  7596,\n",
       "       22862,  6637,  2005,  1996,  1001,  4029,  1025,  2184,  2454,\n",
       "        2019, 22740,  1060,  3396,  1010,  1037,  5049,  2005,  1032,\n",
       "        9139,  6787,  4942,  2953, 16313,  2389,  2686,  3462,  1010,\n",
       "        2038,  3985,  2623,  1996,  2034,  1032,  4888,  3058,  2005,\n",
       "        2049, 15371,  7596,  1012,   102,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0])>"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_features['input_ids'][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Exception encountered when calling layer \"bert\" (type TFBertMainLayer).\n\nnot enough values to unpack (expected 2, got 1)\n\nCall arguments received:\n   input_ids=tf.Tensor(shape=(512,), dtype=int64)\n   attention_mask=None\n   token_type_ids=None\n   position_ids=None\n   head_mask=None\n   inputs_embeds=None\n   encoder_hidden_states=None\n   encoder_attention_mask=None\n   past_key_values=None\n   use_cache=None\n   output_attentions=False\n   output_hidden_states=False\n   return_dict=True\n   training=False",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m/Users/tomaxberg/Documents/Learning/KTH/IA150X - KEX/Code/nlp-energy-impact-main/notebooks/Fine-tuning-BERT.ipynb Cell 48'\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/tomaxberg/Documents/Learning/KTH/IA150X%20-%20KEX/Code/nlp-energy-impact-main/notebooks/Fine-tuning-BERT.ipynb#ch0000051?line=0'>1</a>\u001b[0m model(test_features[\u001b[39m'\u001b[39;49m\u001b[39minput_ids\u001b[39;49m\u001b[39m'\u001b[39;49m][\u001b[39m1\u001b[39;49m])\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/tf-metal/lib/python3.8/site-packages/keras/utils/traceback_utils.py:67\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     <a href='file:///Users/tomaxberg/opt/anaconda3/envs/tf-metal/lib/python3.8/site-packages/keras/utils/traceback_utils.py?line=64'>65</a>\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:  \u001b[39m# pylint: disable=broad-except\u001b[39;00m\n\u001b[1;32m     <a href='file:///Users/tomaxberg/opt/anaconda3/envs/tf-metal/lib/python3.8/site-packages/keras/utils/traceback_utils.py?line=65'>66</a>\u001b[0m   filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n\u001b[0;32m---> <a href='file:///Users/tomaxberg/opt/anaconda3/envs/tf-metal/lib/python3.8/site-packages/keras/utils/traceback_utils.py?line=66'>67</a>\u001b[0m   \u001b[39mraise\u001b[39;00m e\u001b[39m.\u001b[39mwith_traceback(filtered_tb) \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39m\n\u001b[1;32m     <a href='file:///Users/tomaxberg/opt/anaconda3/envs/tf-metal/lib/python3.8/site-packages/keras/utils/traceback_utils.py?line=67'>68</a>\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[1;32m     <a href='file:///Users/tomaxberg/opt/anaconda3/envs/tf-metal/lib/python3.8/site-packages/keras/utils/traceback_utils.py?line=68'>69</a>\u001b[0m   \u001b[39mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/tf-metal/lib/python3.8/site-packages/transformers/modeling_tf_utils.py:383\u001b[0m, in \u001b[0;36munpack_inputs.<locals>.run_call_with_unpacked_inputs\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    <a href='file:///Users/tomaxberg/opt/anaconda3/envs/tf-metal/lib/python3.8/site-packages/transformers/modeling_tf_utils.py?line=380'>381</a>\u001b[0m main_input \u001b[39m=\u001b[39m fn_args_and_kwargs\u001b[39m.\u001b[39mpop(main_input_name, \u001b[39mNone\u001b[39;00m)\n\u001b[1;32m    <a href='file:///Users/tomaxberg/opt/anaconda3/envs/tf-metal/lib/python3.8/site-packages/transformers/modeling_tf_utils.py?line=381'>382</a>\u001b[0m unpacked_inputs \u001b[39m=\u001b[39m input_processing(func, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconfig, main_input, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mfn_args_and_kwargs)\n\u001b[0;32m--> <a href='file:///Users/tomaxberg/opt/anaconda3/envs/tf-metal/lib/python3.8/site-packages/transformers/modeling_tf_utils.py?line=382'>383</a>\u001b[0m \u001b[39mreturn\u001b[39;00m func(\u001b[39mself\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49munpacked_inputs)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/tf-metal/lib/python3.8/site-packages/transformers/models/bert/modeling_tf_bert.py:1633\u001b[0m, in \u001b[0;36mTFBertForSequenceClassification.call\u001b[0;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, output_attentions, output_hidden_states, return_dict, labels, training)\u001b[0m\n\u001b[1;32m   <a href='file:///Users/tomaxberg/opt/anaconda3/envs/tf-metal/lib/python3.8/site-packages/transformers/models/bert/modeling_tf_bert.py?line=1604'>1605</a>\u001b[0m \u001b[39m@unpack_inputs\u001b[39m\n\u001b[1;32m   <a href='file:///Users/tomaxberg/opt/anaconda3/envs/tf-metal/lib/python3.8/site-packages/transformers/models/bert/modeling_tf_bert.py?line=1605'>1606</a>\u001b[0m \u001b[39m@add_start_docstrings_to_model_forward\u001b[39m(BERT_INPUTS_DOCSTRING\u001b[39m.\u001b[39mformat(\u001b[39m\"\u001b[39m\u001b[39mbatch_size, sequence_length\u001b[39m\u001b[39m\"\u001b[39m))\n\u001b[1;32m   <a href='file:///Users/tomaxberg/opt/anaconda3/envs/tf-metal/lib/python3.8/site-packages/transformers/models/bert/modeling_tf_bert.py?line=1606'>1607</a>\u001b[0m \u001b[39m@add_code_sample_docstrings\u001b[39m(\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   <a href='file:///Users/tomaxberg/opt/anaconda3/envs/tf-metal/lib/python3.8/site-packages/transformers/models/bert/modeling_tf_bert.py?line=1624'>1625</a>\u001b[0m     training: Optional[\u001b[39mbool\u001b[39m] \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m,\n\u001b[1;32m   <a href='file:///Users/tomaxberg/opt/anaconda3/envs/tf-metal/lib/python3.8/site-packages/transformers/models/bert/modeling_tf_bert.py?line=1625'>1626</a>\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Union[TFSequenceClassifierOutput, Tuple[tf\u001b[39m.\u001b[39mTensor]]:\n\u001b[1;32m   <a href='file:///Users/tomaxberg/opt/anaconda3/envs/tf-metal/lib/python3.8/site-packages/transformers/models/bert/modeling_tf_bert.py?line=1626'>1627</a>\u001b[0m     \u001b[39mr\u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m   <a href='file:///Users/tomaxberg/opt/anaconda3/envs/tf-metal/lib/python3.8/site-packages/transformers/models/bert/modeling_tf_bert.py?line=1627'>1628</a>\u001b[0m \u001b[39m    labels (`tf.Tensor` or `np.ndarray` of shape `(batch_size,)`, *optional*):\u001b[39;00m\n\u001b[1;32m   <a href='file:///Users/tomaxberg/opt/anaconda3/envs/tf-metal/lib/python3.8/site-packages/transformers/models/bert/modeling_tf_bert.py?line=1628'>1629</a>\u001b[0m \u001b[39m        Labels for computing the sequence classification/regression loss. Indices should be in `[0, ...,\u001b[39;00m\n\u001b[1;32m   <a href='file:///Users/tomaxberg/opt/anaconda3/envs/tf-metal/lib/python3.8/site-packages/transformers/models/bert/modeling_tf_bert.py?line=1629'>1630</a>\u001b[0m \u001b[39m        config.num_labels - 1]`. If `config.num_labels == 1` a regression loss is computed (Mean-Square loss), If\u001b[39;00m\n\u001b[1;32m   <a href='file:///Users/tomaxberg/opt/anaconda3/envs/tf-metal/lib/python3.8/site-packages/transformers/models/bert/modeling_tf_bert.py?line=1630'>1631</a>\u001b[0m \u001b[39m        `config.num_labels > 1` a classification loss is computed (Cross-Entropy).\u001b[39;00m\n\u001b[1;32m   <a href='file:///Users/tomaxberg/opt/anaconda3/envs/tf-metal/lib/python3.8/site-packages/transformers/models/bert/modeling_tf_bert.py?line=1631'>1632</a>\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> <a href='file:///Users/tomaxberg/opt/anaconda3/envs/tf-metal/lib/python3.8/site-packages/transformers/models/bert/modeling_tf_bert.py?line=1632'>1633</a>\u001b[0m     outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbert(\n\u001b[1;32m   <a href='file:///Users/tomaxberg/opt/anaconda3/envs/tf-metal/lib/python3.8/site-packages/transformers/models/bert/modeling_tf_bert.py?line=1633'>1634</a>\u001b[0m         input_ids\u001b[39m=\u001b[39;49minput_ids,\n\u001b[1;32m   <a href='file:///Users/tomaxberg/opt/anaconda3/envs/tf-metal/lib/python3.8/site-packages/transformers/models/bert/modeling_tf_bert.py?line=1634'>1635</a>\u001b[0m         attention_mask\u001b[39m=\u001b[39;49mattention_mask,\n\u001b[1;32m   <a href='file:///Users/tomaxberg/opt/anaconda3/envs/tf-metal/lib/python3.8/site-packages/transformers/models/bert/modeling_tf_bert.py?line=1635'>1636</a>\u001b[0m         token_type_ids\u001b[39m=\u001b[39;49mtoken_type_ids,\n\u001b[1;32m   <a href='file:///Users/tomaxberg/opt/anaconda3/envs/tf-metal/lib/python3.8/site-packages/transformers/models/bert/modeling_tf_bert.py?line=1636'>1637</a>\u001b[0m         position_ids\u001b[39m=\u001b[39;49mposition_ids,\n\u001b[1;32m   <a href='file:///Users/tomaxberg/opt/anaconda3/envs/tf-metal/lib/python3.8/site-packages/transformers/models/bert/modeling_tf_bert.py?line=1637'>1638</a>\u001b[0m         head_mask\u001b[39m=\u001b[39;49mhead_mask,\n\u001b[1;32m   <a href='file:///Users/tomaxberg/opt/anaconda3/envs/tf-metal/lib/python3.8/site-packages/transformers/models/bert/modeling_tf_bert.py?line=1638'>1639</a>\u001b[0m         inputs_embeds\u001b[39m=\u001b[39;49minputs_embeds,\n\u001b[1;32m   <a href='file:///Users/tomaxberg/opt/anaconda3/envs/tf-metal/lib/python3.8/site-packages/transformers/models/bert/modeling_tf_bert.py?line=1639'>1640</a>\u001b[0m         output_attentions\u001b[39m=\u001b[39;49moutput_attentions,\n\u001b[1;32m   <a href='file:///Users/tomaxberg/opt/anaconda3/envs/tf-metal/lib/python3.8/site-packages/transformers/models/bert/modeling_tf_bert.py?line=1640'>1641</a>\u001b[0m         output_hidden_states\u001b[39m=\u001b[39;49moutput_hidden_states,\n\u001b[1;32m   <a href='file:///Users/tomaxberg/opt/anaconda3/envs/tf-metal/lib/python3.8/site-packages/transformers/models/bert/modeling_tf_bert.py?line=1641'>1642</a>\u001b[0m         return_dict\u001b[39m=\u001b[39;49mreturn_dict,\n\u001b[1;32m   <a href='file:///Users/tomaxberg/opt/anaconda3/envs/tf-metal/lib/python3.8/site-packages/transformers/models/bert/modeling_tf_bert.py?line=1642'>1643</a>\u001b[0m         training\u001b[39m=\u001b[39;49mtraining,\n\u001b[1;32m   <a href='file:///Users/tomaxberg/opt/anaconda3/envs/tf-metal/lib/python3.8/site-packages/transformers/models/bert/modeling_tf_bert.py?line=1643'>1644</a>\u001b[0m     )\n\u001b[1;32m   <a href='file:///Users/tomaxberg/opt/anaconda3/envs/tf-metal/lib/python3.8/site-packages/transformers/models/bert/modeling_tf_bert.py?line=1644'>1645</a>\u001b[0m     pooled_output \u001b[39m=\u001b[39m outputs[\u001b[39m1\u001b[39m]\n\u001b[1;32m   <a href='file:///Users/tomaxberg/opt/anaconda3/envs/tf-metal/lib/python3.8/site-packages/transformers/models/bert/modeling_tf_bert.py?line=1645'>1646</a>\u001b[0m     pooled_output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdropout(inputs\u001b[39m=\u001b[39mpooled_output, training\u001b[39m=\u001b[39mtraining)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/tf-metal/lib/python3.8/site-packages/transformers/modeling_tf_utils.py:383\u001b[0m, in \u001b[0;36munpack_inputs.<locals>.run_call_with_unpacked_inputs\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    <a href='file:///Users/tomaxberg/opt/anaconda3/envs/tf-metal/lib/python3.8/site-packages/transformers/modeling_tf_utils.py?line=380'>381</a>\u001b[0m main_input \u001b[39m=\u001b[39m fn_args_and_kwargs\u001b[39m.\u001b[39mpop(main_input_name, \u001b[39mNone\u001b[39;00m)\n\u001b[1;32m    <a href='file:///Users/tomaxberg/opt/anaconda3/envs/tf-metal/lib/python3.8/site-packages/transformers/modeling_tf_utils.py?line=381'>382</a>\u001b[0m unpacked_inputs \u001b[39m=\u001b[39m input_processing(func, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconfig, main_input, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mfn_args_and_kwargs)\n\u001b[0;32m--> <a href='file:///Users/tomaxberg/opt/anaconda3/envs/tf-metal/lib/python3.8/site-packages/transformers/modeling_tf_utils.py?line=382'>383</a>\u001b[0m \u001b[39mreturn\u001b[39;00m func(\u001b[39mself\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49munpacked_inputs)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/tf-metal/lib/python3.8/site-packages/transformers/models/bert/modeling_tf_bert.py:754\u001b[0m, in \u001b[0;36mTFBertMainLayer.call\u001b[0;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict, training)\u001b[0m\n\u001b[1;32m    <a href='file:///Users/tomaxberg/opt/anaconda3/envs/tf-metal/lib/python3.8/site-packages/transformers/models/bert/modeling_tf_bert.py?line=750'>751</a>\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    <a href='file:///Users/tomaxberg/opt/anaconda3/envs/tf-metal/lib/python3.8/site-packages/transformers/models/bert/modeling_tf_bert.py?line=751'>752</a>\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mYou have to specify either input_ids or inputs_embeds\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m--> <a href='file:///Users/tomaxberg/opt/anaconda3/envs/tf-metal/lib/python3.8/site-packages/transformers/models/bert/modeling_tf_bert.py?line=753'>754</a>\u001b[0m batch_size, seq_length \u001b[39m=\u001b[39m input_shape\n\u001b[1;32m    <a href='file:///Users/tomaxberg/opt/anaconda3/envs/tf-metal/lib/python3.8/site-packages/transformers/models/bert/modeling_tf_bert.py?line=755'>756</a>\u001b[0m \u001b[39mif\u001b[39;00m past_key_values \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    <a href='file:///Users/tomaxberg/opt/anaconda3/envs/tf-metal/lib/python3.8/site-packages/transformers/models/bert/modeling_tf_bert.py?line=756'>757</a>\u001b[0m     past_key_values_length \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m\n",
      "\u001b[0;31mValueError\u001b[0m: Exception encountered when calling layer \"bert\" (type TFBertMainLayer).\n\nnot enough values to unpack (expected 2, got 1)\n\nCall arguments received:\n   input_ids=tf.Tensor(shape=(512,), dtype=int64)\n   attention_mask=None\n   token_type_ids=None\n   position_ids=None\n   head_mask=None\n   inputs_embeds=None\n   encoder_hidden_states=None\n   encoder_attention_mask=None\n   past_key_values=None\n   use_cache=None\n   output_attentions=False\n   output_hidden_states=False\n   return_dict=True\n   training=False"
     ]
    }
   ],
   "source": [
    "model(test_features['input_ids'][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "9a9fc1e0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(1, 4), dtype=float32, numpy=\n",
       "array([[ 0.74708337, -1.6014335 ,  2.7002594 , -2.2938297 ]],\n",
       "      dtype=float32)>"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model(input_text_tokenized)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "583742da",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-04-28 14:05:14.745709: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:113] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    }
   ],
   "source": [
    "results = model.predict(test_tokenized['input_ids'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0da3d74b",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'results' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/Users/tomaxberg/Documents/Learning/KTH/IA150X - KEX/Code/nlp-energy-impact-main/notebooks/Fine-tuning-BERT.ipynb Cell 44'\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/tomaxberg/Documents/Learning/KTH/IA150X%20-%20KEX/Code/nlp-energy-impact-main/notebooks/Fine-tuning-BERT.ipynb#ch0000043?line=0'>1</a>\u001b[0m results\n",
      "\u001b[0;31mNameError\u001b[0m: name 'results' is not defined"
     ]
    }
   ],
   "source": [
    "tokenized_text = tokenizer.encode(req['instances'][i]['text'], truncation=True, padding=True, return_tensors=\"tf\")\n",
    "            class_names[np.argmax(tf.nn.softmax(model(tokenized_text)[0], axis=1).numpy())]\n",
    "            result.append(class_names[np.argmax(tf.nn.softmax(model(tokenized_text)[0], axis=1).numpy())])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40298e4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = test_features['input_ids'].numpy(), test_dataset['label'] \n",
    "# X, y = load_iris(return_X_y=True)\n",
    "clf = LogisticRegression(solver=\"liblinear\").fit(X, y)\n",
    "roc_auc_score(y, clf.predict_proba(X), multi_class='ovr')"
   ]
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "python3",
   "name": "tf2-gpu.2-6.m89",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/tf2-gpu.2-6:m89"
  },
  "interpreter": {
   "hash": "dd5144eb0aab924551ffe7da9bd3ba8e6e5c92dee22fbb84fd387c3a4f8acd54"
  },
  "kernelspec": {
   "display_name": "Python 3.8.13 ('tf-metal')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
